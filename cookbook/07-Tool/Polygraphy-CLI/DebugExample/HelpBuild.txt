usage: polygraphy debug build [-h] [-v] [-q]
                              [--verbosity VERBOSITY [VERBOSITY ...]]
                              [--silent]
                              [--log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]]
                              [--log-file LOG_FILE] [--check ...]
                              [--fail-code FAIL_CODES [FAIL_CODES ...] |
                              --ignore-fail-code IGNORE_FAIL_CODES
                              [IGNORE_FAIL_CODES ...]]
                              [--fail-regex FAIL_REGEX [FAIL_REGEX ...]]
                              [--show-output | --hide-fail-output]
                              [--artifacts ARTIFACTS [ARTIFACTS ...]]
                              [--art-dir DIR]
                              [--iter-artifact ITER_ARTIFACT_PATH]
                              [--no-remove-intermediate]
                              [--iter-info ITERATION_INFO_PATH] --until UNTIL
                              [--model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}]
                              [--shape-inference]
                              [--no-onnxruntime-shape-inference]
                              [--external-data-dir EXTERNAL_DATA_DIR]
                              [--ignore-external-data] [--fp-to-fp16]
                              [--seed SEED]
                              [--val-range VAL_RANGE [VAL_RANGE ...]]
                              [--int-min INT_MIN] [--int-max INT_MAX]
                              [--float-min FLOAT_MIN] [--float-max FLOAT_MAX]
                              [--iterations NUM]
                              [--load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                              | --data-loader-script DATA_LOADER_SCRIPT]
                              [--data-loader-func-name DATA_LOADER_FUNC_NAME]
                              [--trt-min-shapes TRT_MIN_SHAPES [TRT_MIN_SHAPES ...]]
                              [--trt-opt-shapes TRT_OPT_SHAPES [TRT_OPT_SHAPES ...]]
                              [--trt-max-shapes TRT_MAX_SHAPES [TRT_MAX_SHAPES ...]]
                              [--tf32] [--fp16] [--fp8] [--int8]
                              [--precision-constraints {prefer,obey,none} | --strict-types]
                              [--sparse-weights] [--version-compatible]
                              [--exclude-lean-runtime] [--workspace BYTES]
                              [--calibration-cache CALIBRATION_CACHE]
                              [--calib-base-cls CALIBRATION_BASE_CLASS]
                              [--quantile QUANTILE]
                              [--regression-cutoff REGRESSION_CUTOFF]
                              [--load-timing-cache LOAD_TIMING_CACHE]
                              [--save-tactics SAVE_TACTICS | --load-tactics LOAD_TACTICS]
                              [--tactic-sources [TACTIC_SOURCES ...]]
                              [--trt-config-script TRT_CONFIG_SCRIPT]
                              [--trt-config-func-name TRT_CONFIG_FUNC_NAME]
                              [--trt-config-postprocess-script TRT_CONFIG_POSTPROCESS_SCRIPT]
                              [--trt-safety-restricted] [--refittable]
                              [--use-dla] [--allow-gpu-fallback]
                              [--pool-limit MEMORY_POOL_LIMIT [MEMORY_POOL_LIMIT ...]]
                              [--preview-features [PREVIEW_FEATURES ...]]
                              [--builder-optimization-level BUILDER_OPTIMIZATION_LEVEL]
                              [--hardware-compatibility-level HARDWARE_COMPATIBILITY_LEVEL]
                              [--max-aux-streams MAX_AUX_STREAMS]
                              [--plugins PLUGINS [PLUGINS ...]]
                              [--onnx-flags ONNX_FLAGS [ONNX_FLAGS ...]]
                              [--trt-outputs TRT_OUTPUTS [TRT_OUTPUTS ...]]
                              [--trt-exclude-outputs TRT_EXCLUDE_OUTPUTS [TRT_EXCLUDE_OUTPUTS ...]]
                              [--layer-precisions LAYER_PRECISIONS [LAYER_PRECISIONS ...]]
                              [--tensor-dtypes TENSOR_DTYPES [TENSOR_DTYPES ...]]
                              [--trt-network-func-name TRT_NETWORK_FUNC_NAME]
                              [--trt-network-postprocess-script TRT_NETWORK_POSTPROCESS_SCRIPT [TRT_NETWORK_POSTPROCESS_SCRIPT ...]]
                              [--save-timing-cache SAVE_TIMING_CACHE]
                              model_file

Repeatedly build an engine to isolate faulty tactics.

`debug build` follows the same general process as other `debug` subtools (refer to the help output
of the `debug` tool for more background information and details).

Specifically, it does the following during each iteration:

1. Builds a TensorRT engine and saves it in the current directory as `polygraphy_debug.engine` by default.
2. Evaluates it using the `--check` command if it was provided, or in interactive mode otherwise.
3. Sorts files specified by `--artifacts` into `good` and `bad` directories based on (2).
    This is useful for sorting tactic replays, which can then be further analyzed with `inspect diff-tactics`.

The typical usage of `debug build` is:

    polygraphy debug build <model> [trt_build_options...] [--save-tactics <replay_file>] \
        [--artifacts <replay_file>] --until <num_iterations> \
        [--check <check_command>]

`polygraphy run` is usually a good choice for the `--check` command.

options:
  -h, --help            show this help message and exit

Logging:
  Options related to logging and debug output

  -v, --verbose         Increase logging verbosity. Specify multiple times for
                        higher verbosity
  -q, --quiet           Decrease logging verbosity. Specify multiple times for
                        lower verbosity
  --verbosity VERBOSITY [VERBOSITY ...]
                        The logging verbosity to use. Takes precedence over
                        the `-v` and `-q` options, and unlike them, allows you
                        to control per-path verbosity. Verbosity values should
                        come from Polygraphy's logging verbosities defined in
                        the `Logger` class and are case-insensitive. For
                        example: `--verbosity INFO` or `--verbosity verbose`.
                        To specify per-path verbosity, use the format:
                        `<path>:<verbosity>`. For example: `--verbosity
                        backend/trt:INFO backend/trt/loader.py:VERBOSE`. Paths
                        should be relative to the `polygraphy/` directory. For
                        example, `polygraphy/backend` should be specified with
                        just `backend`. The most closely matching path is used
                        to determine verbosity. For example, with:
                        `--verbosity warning backend:info
                        backend/trt:verbose`, a file under
                        `polygraphy/comparator` would use `WARNING` verbosity,
                        one under `backend/onnx` would use `INFO`, and one
                        under `backend/trt` would use `VERBOSE`.
  --silent              Disable all output
  --log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]
                        Format for log messages: {{'timestamp': Include
                        timestamp, 'line-info': Include file and line number,
                        'no-colors': Disable colors}}
  --log-file LOG_FILE   Path to a file where Polygraphy logging output should
                        be written. This may not include logging output from
                        dependencies, like TensorRT or ONNX-Runtime.

Pass/Fail Reporting:
  Options related to reporting pass/fail status during iterative debugging.

  --check ..., --check-inference ...
                        A command to check the model. When this is omitted, an
                        interactive debugging session is started instead.By
                        default an exit status of 0 is treated as a 'pass'
                        whereas any other exit status is treated as a 'fail'.
  --fail-code FAIL_CODES [FAIL_CODES ...], --fail-returncode FAIL_CODES [FAIL_CODES ...]
                        The return code(s) from the --check command to count
                        as failures. If this is provided, any other return
                        code will be counted as a success.
  --ignore-fail-code IGNORE_FAIL_CODES [IGNORE_FAIL_CODES ...], --ignore-fail-returncode IGNORE_FAIL_CODES [IGNORE_FAIL_CODES ...]
                        The return code(s) from the --check command to ignore
                        as failures.
  --fail-regex FAIL_REGEX [FAIL_REGEX ...]
                        Regular expression denoting an error in the check
                        command's output. The command is only considered a
                        failure if a matching string is found in the command's
                        output. This can be useful to distinguish among
                        multiple types of failures. Can be specified multiple
                        times to match different regular expressions, in which
                        case any match counts as a failure. When combined with
                        --fail-code, only iterations whose return code is
                        considered a failure are checked for regular
                        expressions.
  --show-output         Show output from the --check command even for passing
                        iterations. By default, output from passing iterations
                        is captured.
  --hide-fail-output    Suppress output from the --check command for failing
                        iterations. By default, output from failing iterations
                        is displayed.

Artifact Sorting:
  Options related to sorting artifacts into good/bad directories based on pass/fail status.

  --artifacts ARTIFACTS [ARTIFACTS ...]
                        Path(s) of artifacts to sort. These will be moved into
                        'good' and 'bad' directories based on the exit status
                        of the `--check` command and suffixed with an
                        iteration number, timestamp and return code.
  --art-dir DIR, --artifacts-dir DIR
                        The directory in which to move artifacts and sort them
                        into 'good' and 'bad'. Defaults to a directory named
                        `polygraphy_artifacts` in the current directory.

Iterative Debugging:
  Options related to iteratively debugging.

  --iter-artifact ITER_ARTIFACT_PATH, --intermediate-artifact ITER_ARTIFACT_PATH
                        Path to store the intermediate artifact from each
                        iteration. Defaults to: polygraphy_debug.engine
  --no-remove-intermediate
                        Do not remove the intermediate artifact between
                        iterations. Subsequent iterations may still overwrite
                        the artifact from previous iterations. This allows you
                        to exit the tool early and still have access to the
                        most recent intermediate artifact.
  --iter-info ITERATION_INFO_PATH, --iteration-info ITERATION_INFO_PATH
                        Path to write a JSON file containing information about
                        the current iteration. This will include an
                        'iteration' key whose value is the current iteration
                        number.
  --until UNTIL         Controls when to stop running. Choices are: ['good',
                        'bad', int]. 'good' will keep running until the first
                        'good' run. 'bad' will run until the first 'bad' run.
                        An integer can be specified to run a set number of
                        iterations.

Model:
  Options related to the model

  model_file            Path to the model
  --model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}
                        The type of the input model: {{'frozen': TensorFlow
                        frozen graph; 'keras': Keras model; 'ckpt': TensorFlow
                        checkpoint directory; 'onnx': ONNX model; 'engine':
                        TensorRT engine; 'trt-network-script': A Python script
                        that defines a `load_network` function that takes no
                        arguments and returns a TensorRT Builder, Network, and
                        optionally Parser. If the function name is not
                        `load_network`, it can be specified after the model
                        file, separated by a colon. For example:
                        `my_custom_script.py:my_func`; 'uff': UFF file
                        [deprecated]; 'caffe': Caffe prototxt [deprecated]}}

ONNX Shape Inference:
  Options related to ONNX shape inference.

  --shape-inference, --do-shape-inference
                        Enable ONNX shape inference when loading the model
  --no-onnxruntime-shape-inference
                        Disable using ONNX-Runtime's shape inference
                        utilities. This will force Polygraphy to use
                        `onnx.shape_inference` instead. Note that ONNX-
                        Runtime's shape inference utilities may be more
                        performant and memory-efficient.

ONNX Model Loading:
  Options related to loading ONNX models.

  --external-data-dir EXTERNAL_DATA_DIR, --load-external-data EXTERNAL_DATA_DIR, --ext EXTERNAL_DATA_DIR
                        Path to a directory containing external data for the
                        model. Generally, this is only required if the
                        external data is not stored in the model directory.
  --ignore-external-data
                        Ignore external data and just load the model structure
                        without any weights. The model will be usable only for
                        purposes that don't require weights, such as
                        extracting subgraphs or inspecting model structure.
                        This can be useful in cases where external data is not
                        available.
  --fp-to-fp16          Convert all floating point tensors in an ONNX model to
                        16-bit precision. This is *not* needed in order to use
                        TensorRT's fp16 precision, but may be useful for other
                        backends. Requires onnxmltools.

Data Loader:
  Options related to loading or generating input data for inference.

  --seed SEED           Seed to use for random inputs
  --val-range VAL_RANGE [VAL_RANGE ...]
                        Range of values to generate in the data loader. To
                        specify per-input ranges, use the format: --val-range
                        <input_name>:[min,max]. If no input name is provided,
                        the range is used for any inputs not explicitly
                        specified. For example: --val-range [0,1] inp0:[2,50]
                        inp1:[3.0,4.6]
  --int-min INT_MIN     [DEPRECATED: Use --val-range] Minimum integer value
                        for random integer inputs
  --int-max INT_MAX     [DEPRECATED: Use --val-range] Maximum integer value
                        for random integer inputs
  --float-min FLOAT_MIN
                        [DEPRECATED: Use --val-range] Minimum float value for
                        random float inputs
  --float-max FLOAT_MAX
                        [DEPRECATED: Use --val-range] Maximum float value for
                        random float inputs
  --iterations NUM, --iters NUM
                        Number of inference iterations for which the default
                        data loader should supply data
  --load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...], --load-input-data LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                        Path(s) to load inputs. The file(s) should be a JSON-
                        ified List[Dict[str, numpy.ndarray]], i.e. a list
                        where each element is the feed_dict for a single
                        iteration. When this option is used, all other data
                        loader arguments are ignored.
  --data-loader-script DATA_LOADER_SCRIPT
                        Path to a Python script that defines a function that
                        loads input data. The function should take no
                        arguments and return a generator or iterable that
                        yields input data (Dict[str, np.ndarray]). When this
                        option is used, all other data loader arguments are
                        ignored. By default, Polygraphy looks for a function
                        called `load_data`. You can specify a custom function
                        name by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --data-loader-func-name DATA_LOADER_FUNC_NAME
                        [DEPRECATED - function name can be specified with
                        --data-loader-script like so:
                        `my_custom_script.py:my_func`] When using a data-
                        loader-script, this specifies the name of the function
                        that loads data. Defaults to `load_data`.

TensorRT Builder Configuration:
  Options related to creating the TensorRT BuilderConfig.

  --trt-min-shapes TRT_MIN_SHAPES [TRT_MIN_SHAPES ...]
                        The minimum shapes the optimization profile(s) will
                        support. Specify this option once for each profile. If
                        not provided, inference-time input shapes are used.
                        Format: --trt-min-shapes <input0>:[D0,D1,..,DN] ..
                        <inputN>:[D0,D1,..,DN]
  --trt-opt-shapes TRT_OPT_SHAPES [TRT_OPT_SHAPES ...]
                        The shapes for which the optimization profile(s) will
                        be most performant. Specify this option once for each
                        profile. If not provided, inference-time input shapes
                        are used. Format: --trt-opt-shapes
                        <input0>:[D0,D1,..,DN] .. <inputN>:[D0,D1,..,DN]
  --trt-max-shapes TRT_MAX_SHAPES [TRT_MAX_SHAPES ...]
                        The maximum shapes the optimization profile(s) will
                        support. Specify this option once for each profile. If
                        not provided, inference-time input shapes are used.
                        Format: --trt-max-shapes <input0>:[D0,D1,..,DN] ..
                        <inputN>:[D0,D1,..,DN]
  --tf32                Enable tf32 precision in TensorRT
  --fp16                Enable fp16 precision in TensorRT
  --fp8                 Enable fp8 precision in TensorRT
  --int8                Enable int8 precision in TensorRT. If calibration is
                        required but no calibration cache is provided, this
                        option will cause TensorRT to run int8 calibration
                        using the Polygraphy data loader to provide
                        calibration data.
  --precision-constraints {prefer,obey,none}
                        If set to `prefer`, TensorRT will restrict available
                        tactics to layer precisions specified in the network
                        unless no implementation exists with the preferred
                        layer constraints, in which case it will issue a
                        warning and use the fastest available implementation.
                        If set to `obey`, TensorRT will instead fail to build
                        the network if no implementation exists with the
                        preferred layer constraints. Defaults to `none`
  --strict-types        [DEPRECATED - use --precision-constraints] Enable
                        preference for precision constraints and avoidance of
                        I/O reformatting in TensorRT, and fall back to
                        ignoring the request if such an engine cannot be
                        built.
  --sparse-weights      Enable optimizations for sparse weights in TensorRT
  --version-compatible  Builds an engine designed to be forward TensorRT
                        version compatible.
  --exclude-lean-runtime
                        Exclude the lean runtime from the plan when version
                        compatibility is enabled.
  --workspace BYTES     [DEPRECATED - use --pool-limit] Amount of memory, in
                        bytes, to allocate for the TensorRT builder's
                        workspace. Optionally, use a `K`, `M`, or `G` suffix
                        to indicate KiB, MiB, or GiB respectively. For
                        example, `--workspace=16M` is equivalent to
                        `--workspace=16777216`.
  --calibration-cache CALIBRATION_CACHE
                        Path to load/save a calibration cache. Used to store
                        calibration scales to speed up the process of int8
                        calibration. If the provided path does not yet exist,
                        int8 calibration scales will be calculated and written
                        to it during engine building. If the provided path
                        does exist, it will be read and int8 calibration will
                        be skipped during engine building.
  --calib-base-cls CALIBRATION_BASE_CLASS, --calibration-base-class CALIBRATION_BASE_CLASS
                        The name of the calibration base class to use. For
                        example, 'IInt8MinMaxCalibrator'.
  --quantile QUANTILE   The quantile to use for IInt8LegacyCalibrator. Has no
                        effect for other calibrator types.
  --regression-cutoff REGRESSION_CUTOFF
                        The regression cutoff to use for
                        IInt8LegacyCalibrator. Has no effect for other
                        calibrator types.
  --load-timing-cache LOAD_TIMING_CACHE
                        Path to load tactic timing cache. Used to cache tactic
                        timing information to speed up the engine building
                        process. If the file specified by --load-timing-cache
                        does not exist, Polygraphy will emit a warning and
                        fall back to using an empty timing cache.
  --save-tactics SAVE_TACTICS, --save-tactic-replay SAVE_TACTICS
                        Path to save a Polygraphy tactic replay file. Details
                        about tactics selected by TensorRT will be recorded
                        and stored at this location as a JSON file.
  --load-tactics LOAD_TACTICS, --load-tactic-replay LOAD_TACTICS
                        Path to load a Polygraphy tactic replay file, such as
                        one created by --save-tactics. The tactics specified
                        in the file will be used to override TensorRT's
                        default selections.
  --tactic-sources [TACTIC_SOURCES ...]
                        Tactic sources to enable. This controls which
                        libraries (e.g. cudnn, cublas, etc.) TensorRT is
                        allowed to load tactics from. Values come from the
                        names of the values in the trt.TacticSource enum and
                        are case-insensitive. If no arguments are provided,
                        e.g. '--tactic-sources', then all tactic sources are
                        disabled.Defaults to TensorRT's default tactic
                        sources.
  --trt-config-script TRT_CONFIG_SCRIPT
                        Path to a Python script that defines a function that
                        creates a TensorRT IBuilderConfig. The function should
                        take a builder and network as parameters and return a
                        TensorRT builder configuration. When this option is
                        specified, all other config arguments are ignored. By
                        default, Polygraphy looks for a function called
                        `load_config`. You can specify a custom function name
                        by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --trt-config-func-name TRT_CONFIG_FUNC_NAME
                        [DEPRECATED - function name can be specified with
                        --trt-config-script like so:
                        `my_custom_script.py:my_func`]When using a trt-config-
                        script, this specifies the name of the function that
                        creates the config. Defaults to `load_config`.
  --trt-config-postprocess-script TRT_CONFIG_POSTPROCESS_SCRIPT, --trt-cpps TRT_CONFIG_POSTPROCESS_SCRIPT
                        [EXPERIMENTAL] Path to a Python script that defines a
                        function that modifies a TensorRT IBuilderConfig. This
                        function will be called after Polygraphy has finished
                        created the builder configuration and should take a
                        builder, network, and config as parameters and modify
                        the config in place. Unlike `--trt-config-script`, all
                        other config arguments will be reflected in the config
                        passed to the function.By default, Polygraphy looks
                        for a function called `postprocess_config`. You can
                        specify a custom function name by separating it with a
                        colon. For example: `my_custom_script.py:my_func`
  --trt-safety-restricted
                        Enable safety scope checking in TensorRT
  --refittable          Enable the engine to be refitted with new weights
                        after it is built.
  --use-dla             [EXPERIMENTAL] Use DLA as the default device type
  --allow-gpu-fallback  [EXPERIMENTAL] Allow layers unsupported on the DLA to
                        fall back to GPU. Has no effect if --use-dla is not
                        set.
  --pool-limit MEMORY_POOL_LIMIT [MEMORY_POOL_LIMIT ...], --memory-pool-limit MEMORY_POOL_LIMIT [MEMORY_POOL_LIMIT ...]
                        Memory pool limits. Memory pool names come from the
                        names of values in the `trt.MemoryPoolType` enum and
                        are case-insensitiveFormat: `--pool-limit
                        <pool_name>:<pool_limit> ...`. For example, `--pool-
                        limit dla_local_dram:1e9 workspace:16777216`.
                        Optionally, use a `K`, `M`, or `G` suffix to indicate
                        KiB, MiB, or GiB respectively. For example, `--pool-
                        limit workspace:16M` is equivalent to `--pool-limit
                        workspace:16777216`.
  --preview-features [PREVIEW_FEATURES ...]
                        Preview features to enable. Values come from the names
                        of the values in the trt.PreviewFeature enum, and are
                        case-insensitive.If no arguments are provided, e.g. '
                        --preview-features', then all preview features are
                        disabled. Defaults to TensorRT's default preview
                        features.
  --builder-optimization-level BUILDER_OPTIMIZATION_LEVEL
                        The builder optimization level. Setting a higher
                        optimization level allows the optimizer to spend more
                        time searching for optimization opportunities. The
                        resulting engine may have better performance compared
                        to an engine built with a lower optimization level.
                        Refer to the TensorRT API documentation for details.
  --hardware-compatibility-level HARDWARE_COMPATIBILITY_LEVEL
                        The hardware compatibility level to use for the
                        engine. This allows engines built on one GPU
                        architecture to work on GPUs of other architectures.
                        Values come from the names of values in the
                        `trt.HardwareCompatibilityLevel` enum and are case-
                        insensitive. For example, `--hardware-compatibility-
                        level ampere_plus`
  --max-aux-streams MAX_AUX_STREAMS
                        The maximum number of auxiliary streams that TensorRT
                        is allowed to use. If the network contains operators
                        that can run in parallel, TRT can execute them using
                        auxiliary streams in addition to the one provided to
                        the IExecutionContext.execute_async_v3() call. The
                        default maximum number of auxiliary streams is
                        determined by the heuristics in TensorRT on whether
                        enabling multi-stream would improve the performance.
                        Refer to the TensorRT API documentation for details.

TensorRT Plugin Loading:
  Options related to loading TensorRT plugins.

  --plugins PLUGINS [PLUGINS ...]
                        Path(s) of plugin libraries to load

ONNX-TRT Parser Flags:
  Options related to setting flags for TensorRT's ONNX parser

  --onnx-flags ONNX_FLAGS [ONNX_FLAGS ...]
                        Flag(s) for adjusting the default parsing behavior of
                        the ONNX parser.Flag values come from the
                        `trt.OnnxParserFlag` enum and are case-insensitve.For
                        example: --onnx-flags native_instancenorm

TensorRT Network Loading:
  Options related to loading TensorRT networks.

  --trt-outputs TRT_OUTPUTS [TRT_OUTPUTS ...]
                        Name(s) of TensorRT output(s). Using '--trt-outputs
                        mark all' indicates that all tensors should be used as
                        outputs
  --trt-exclude-outputs TRT_EXCLUDE_OUTPUTS [TRT_EXCLUDE_OUTPUTS ...]
                        [EXPERIMENTAL] Name(s) of TensorRT output(s) to unmark
                        as outputs.
  --layer-precisions LAYER_PRECISIONS [LAYER_PRECISIONS ...]
                        Compute precision to use for each layer. This should
                        be specified on a per-layer basis, using the format:
                        --layer-precisions <layer_name>:<layer_precision>.
                        Precision values come from the TensorRT data type
                        aliases, like float32, float16, int8, bool, etc. For
                        example: --layer-precisions example_layer:float16
                        other_layer:int8. When this option is provided, you
                        should also set --precision-constraints to either
                        'prefer' or 'obey'.
  --tensor-dtypes TENSOR_DTYPES [TENSOR_DTYPES ...], --tensor-datatypes TENSOR_DTYPES [TENSOR_DTYPES ...]
                        Data type to use for each tensor. This should be
                        specified on a per-tensor basis, using the format:
                        --tensor-datatypes <tensor_name>:<tensor_datatype>.
                        Data type values come from the TensorRT data type
                        aliases, like float32, float16, int8, bool, etc. For
                        example: --tensor-datatypes example_tensor:float16
                        other_tensor:int8.
  --trt-network-func-name TRT_NETWORK_FUNC_NAME
                        [DEPRECATED - function name can be specified alongside
                        the script like so: `my_custom_script.py:my_func`]
                        When using a trt-network-script instead of other model
                        types, this specifies the name of the function that
                        loads the network. Defaults to `load_network`.
  --trt-network-postprocess-script TRT_NETWORK_POSTPROCESS_SCRIPT [TRT_NETWORK_POSTPROCESS_SCRIPT ...], --trt-npps TRT_NETWORK_POSTPROCESS_SCRIPT [TRT_NETWORK_POSTPROCESS_SCRIPT ...]
                        [EXPERIMENTAL] Specify a post-processing script to run
                        on the parsed TensorRT network. The script file may
                        optionally be suffixed with the name of the callable
                        to be invoked. For example: `--trt-npps
                        process.py:do_something`. If no callable is specified,
                        then by default Polygraphy uses the callable name
                        `postprocess`. The callable is expected to take a
                        named argument `network` of type
                        `trt.INetworkDefinition`. Multiple scripts may be
                        specified, in which case they are executed in the
                        order given.

TensorRT Engine:
  Options related to loading or building TensorRT engines.

  --save-timing-cache SAVE_TIMING_CACHE
                        Path to save tactic timing cache if building an
                        engine. Existing caches will be appended to with any
                        new timing information gathered.
