usage: polygraphy debug precision [-h] [-v] [-q]
                                  [--verbosity VERBOSITY [VERBOSITY ...]]
                                  [--silent]
                                  [--log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]]
                                  [--log-file LOG_FILE] [--check ...]
                                  [--fail-code FAIL_CODES [FAIL_CODES ...] |
                                  --ignore-fail-code IGNORE_FAIL_CODES
                                  [IGNORE_FAIL_CODES ...]]
                                  [--fail-regex FAIL_REGEX [FAIL_REGEX ...]]
                                  [--show-output | --hide-fail-output]
                                  [--artifacts ARTIFACTS [ARTIFACTS ...]]
                                  [--art-dir DIR]
                                  [--iter-artifact ITER_ARTIFACT_PATH]
                                  [--no-remove-intermediate]
                                  [--iter-info ITERATION_INFO_PATH]
                                  [--load-debug-replay LOAD_DEBUG_REPLAY]
                                  [--save-debug-replay SAVE_DEBUG_REPLAY]
                                  [--model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}]
                                  [--shape-inference]
                                  [--no-onnxruntime-shape-inference]
                                  [--external-data-dir EXTERNAL_DATA_DIR]
                                  [--ignore-external-data] [--fp-to-fp16]
                                  [--seed SEED]
                                  [--val-range VAL_RANGE [VAL_RANGE ...]]
                                  [--int-min INT_MIN] [--int-max INT_MAX]
                                  [--float-min FLOAT_MIN]
                                  [--float-max FLOAT_MAX] [--iterations NUM]
                                  [--data-loader-backend-module {numpy,torch}]
                                  [--load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                                  | --data-loader-script DATA_LOADER_SCRIPT]
                                  [--data-loader-func-name DATA_LOADER_FUNC_NAME]
                                  [--trt-min-shapes TRT_MIN_SHAPES [TRT_MIN_SHAPES ...]]
                                  [--trt-opt-shapes TRT_OPT_SHAPES [TRT_OPT_SHAPES ...]]
                                  [--trt-max-shapes TRT_MAX_SHAPES [TRT_MAX_SHAPES ...]]
                                  [--tf32] [--fp16] [--bf16] [--fp8] [--int8]
                                  [--precision-constraints {prefer,obey,none}]
                                  [--sparse-weights] [--version-compatible]
                                  [--exclude-lean-runtime]
                                  [--calibration-cache CALIBRATION_CACHE]
                                  [--calib-base-cls CALIBRATION_BASE_CLASS]
                                  [--quantile QUANTILE]
                                  [--regression-cutoff REGRESSION_CUTOFF]
                                  [--load-timing-cache LOAD_TIMING_CACHE]
                                  [--error-on-timing-cache-miss]
                                  [--disable-compilation-cache]
                                  [--save-tactics SAVE_TACTICS | --load-tactics LOAD_TACTICS]
                                  [--tactic-sources [TACTIC_SOURCES ...]]
                                  [--trt-config-script TRT_CONFIG_SCRIPT]
                                  [--trt-config-func-name TRT_CONFIG_FUNC_NAME]
                                  [--trt-config-postprocess-script TRT_CONFIG_POSTPROCESS_SCRIPT]
                                  [--trt-safety-restricted] [--refittable]
                                  [--strip-plan] [--use-dla]
                                  [--allow-gpu-fallback]
                                  [--pool-limit MEMORY_POOL_LIMIT [MEMORY_POOL_LIMIT ...]]
                                  [--preview-features [PREVIEW_FEATURES ...]]
                                  [--builder-optimization-level BUILDER_OPTIMIZATION_LEVEL]
                                  [--hardware-compatibility-level HARDWARE_COMPATIBILITY_LEVEL]
                                  [--max-aux-streams MAX_AUX_STREAMS]
                                  [--quantization-flags [QUANTIZATION_FLAGS ...]]
                                  [--profiling-verbosity PROFILING_VERBOSITY]
                                  [--weight-streaming]
                                  [--runtime-platform RUNTIME_PLATFORM]
                                  [--plugins PLUGINS [PLUGINS ...]]
                                  [--onnx-flags ONNX_FLAGS [ONNX_FLAGS ...]]
                                  [--plugin-instancenorm]
                                  [--trt-outputs TRT_OUTPUTS [TRT_OUTPUTS ...]]
                                  [--trt-exclude-outputs TRT_EXCLUDE_OUTPUTS [TRT_EXCLUDE_OUTPUTS ...]]
                                  [--layer-precisions LAYER_PRECISIONS [LAYER_PRECISIONS ...]]
                                  [--tensor-dtypes TENSOR_DTYPES [TENSOR_DTYPES ...]]
                                  [--trt-network-func-name TRT_NETWORK_FUNC_NAME]
                                  [--trt-network-postprocess-script TRT_NETWORK_POSTPROCESS_SCRIPT [TRT_NETWORK_POSTPROCESS_SCRIPT ...]]
                                  [--strongly-typed]
                                  [--mark-debug MARK_DEBUG [MARK_DEBUG ...]]
                                  [--save-timing-cache SAVE_TIMING_CACHE]
                                  [--mode {bisect,linear}]
                                  [--dir {forward,reverse}]
                                  [-p {float32,float16}]
                                  model_file

[EXPERIMENTAL] Iteratively mark layers to run in a higher precision to find a compromise between performance and quality.

`debug precision` follows the same general process as other `debug` subtools (refer to the help output
of the `debug` tool for more background information and details).

Each iteration will generate an engine called 'polygraphy_debug.engine' in the current directory.

options:
  -h, --help            show this help message and exit
  --mode {bisect,linear}
                        How layers are selected to run in higher precision.
                        'bisect' will use binary search, and 'linear' will
                        iteratively mark one extra layer at a time
  --dir {forward,reverse}, --direction {forward,reverse}
                        Order in which layers are marked to run in higher
                        precision. 'forward' will start marking layers from
                        network inputs, and 'reverse' will start from the
                        network outputs
  -p {float32,float16}, --precision {float32,float16}
                        Precision to use when marking layers to run in higher
                        precision

Logging:
  Options related to logging and debug output

  -v, --verbose         Increase logging verbosity. Specify multiple times for
                        higher verbosity
  -q, --quiet           Decrease logging verbosity. Specify multiple times for
                        lower verbosity
  --verbosity VERBOSITY [VERBOSITY ...]
                        The logging verbosity to use. Takes precedence over
                        the `-v` and `-q` options, and unlike them, allows you
                        to control per-path verbosity. Verbosity values should
                        come from Polygraphy's logging verbosities defined in
                        the `Logger` class and are case-insensitive. For
                        example: `--verbosity INFO` or `--verbosity verbose`.
                        To specify per-path verbosity, use the format:
                        `<path>:<verbosity>`. For example: `--verbosity
                        backend/trt:INFO backend/trt/loader.py:VERBOSE`. Paths
                        should be relative to the `polygraphy/` directory. For
                        example, `polygraphy/backend` should be specified with
                        just `backend`. The most closely matching path is used
                        to determine verbosity. For example, with:
                        `--verbosity warning backend:info
                        backend/trt:verbose`, a file under
                        `polygraphy/comparator` would use `WARNING` verbosity,
                        one under `backend/onnx` would use `INFO`, and one
                        under `backend/trt` would use `VERBOSE`.
  --silent              Disable all output
  --log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]
                        Format for log messages: {{'timestamp': Include
                        timestamp, 'line-info': Include file and line number,
                        'no-colors': Disable colors}}
  --log-file LOG_FILE   Path to a file where Polygraphy logging output should
                        be written. This may not include logging output from
                        dependencies, like TensorRT or ONNX-Runtime.

Pass/Fail Reporting:
  Options related to reporting pass/fail status during iterative debugging.

  --check ..., --check-inference ...
                        A command to check the model. When this is omitted, an
                        interactive debugging session is started instead.By
                        default an exit status of 0 is treated as a 'pass'
                        whereas any other exit status is treated as a 'fail'.
  --fail-code FAIL_CODES [FAIL_CODES ...], --fail-returncode FAIL_CODES [FAIL_CODES ...]
                        The return code(s) from the --check command to count
                        as failures. If this is provided, any other return
                        code will be counted as a success.
  --ignore-fail-code IGNORE_FAIL_CODES [IGNORE_FAIL_CODES ...], --ignore-fail-returncode IGNORE_FAIL_CODES [IGNORE_FAIL_CODES ...]
                        The return code(s) from the --check command to ignore
                        as failures.
  --fail-regex FAIL_REGEX [FAIL_REGEX ...]
                        Regular expression denoting an error in the check
                        command's output. The command is only considered a
                        failure if a matching string is found in the command's
                        output. This can be useful to distinguish among
                        multiple types of failures. Can be specified multiple
                        times to match different regular expressions, in which
                        case any match counts as a failure. When combined with
                        --fail-code, only iterations whose return code is
                        considered a failure are checked for regular
                        expressions.
  --show-output         Show output from the --check command even for passing
                        iterations. By default, output from passing iterations
                        is captured.
  --hide-fail-output    Suppress output from the --check command for failing
                        iterations. By default, output from failing iterations
                        is displayed.

Artifact Sorting:
  Options related to sorting artifacts into good/bad directories based on pass/fail status.

  --artifacts ARTIFACTS [ARTIFACTS ...]
                        Path(s) of artifacts to sort. These will be moved into
                        'good' and 'bad' directories based on the exit status
                        of the `--check` command and suffixed with an
                        iteration number, timestamp and return code.
  --art-dir DIR, --artifacts-dir DIR
                        The directory in which to move artifacts and sort them
                        into 'good' and 'bad'. Defaults to a directory named
                        `polygraphy_artifacts` in the current directory.

Iterative Debugging:
  Options related to iteratively debugging.

  --iter-artifact ITER_ARTIFACT_PATH, --intermediate-artifact ITER_ARTIFACT_PATH
                        Path to store the intermediate artifact from each
                        iteration. Defaults to: polygraphy_debug.engine
  --no-remove-intermediate
                        Do not remove the intermediate artifact between
                        iterations. Subsequent iterations may still overwrite
                        the artifact from previous iterations. This allows you
                        to exit the tool early and still have access to the
                        most recent intermediate artifact.
  --iter-info ITERATION_INFO_PATH, --iteration-info ITERATION_INFO_PATH
                        Path to write a JSON file containing information about
                        the current iteration. This will include an
                        'iteration' key whose value is the current iteration
                        number.
  --load-debug-replay LOAD_DEBUG_REPLAY
                        Path from which to load a debug replay. A replay file
                        includes information on the results of some or all
                        iterations, allowing you to skip those iterations.
  --save-debug-replay SAVE_DEBUG_REPLAY
                        Path at which to save a debug replay, which includes
                        information on the results of debugging iterations.
                        The replay can be used with `--load-debug-replay` to
                        skip iterations during subsequent debugging sessions.
                        The replay is saved after the first iteration and
                        overwritten with an updated replay during each
                        iteration thereafter. This will also write a second
                        replay file with a suffix of `_skip_current`, which is
                        written before the iteration completes, and treats it
                        as a failure. In cases where the iteration crashes,
                        loading this replay file provides a means of skipping
                        over the crash. Defaults to
                        `polygraphy_debug_replay.json` in the current
                        directory.

Model:
  Options related to the model

  model_file            Path to the model
  --model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}
                        The type of the input model: {{'frozen': TensorFlow
                        frozen graph; 'keras': Keras model; 'ckpt': TensorFlow
                        checkpoint directory; 'onnx': ONNX model; 'engine':
                        TensorRT engine; 'trt-network-script': A Python script
                        that defines a `load_network` function that takes no
                        arguments and returns a TensorRT Builder, Network, and
                        optionally Parser. If the function name is not
                        `load_network`, it can be specified after the model
                        file, separated by a colon. For example:
                        `my_custom_script.py:my_func`; 'uff': UFF file
                        [deprecated]; 'caffe': Caffe prototxt [deprecated]}}

ONNX Shape Inference:
  Options related to ONNX shape inference.

  --shape-inference, --do-shape-inference
                        Enable ONNX shape inference when loading the model
  --no-onnxruntime-shape-inference
                        Disable using ONNX-Runtime's shape inference
                        utilities. This will force Polygraphy to use
                        `onnx.shape_inference` instead. Note that ONNX-
                        Runtime's shape inference utilities may be more
                        performant and memory-efficient.

ONNX Model Loading:
  Options related to loading ONNX models.

  --external-data-dir EXTERNAL_DATA_DIR, --load-external-data EXTERNAL_DATA_DIR, --ext EXTERNAL_DATA_DIR
                        Path to a directory containing external data for the
                        model. Generally, this is only required if the
                        external data is not stored in the model directory.
  --ignore-external-data
                        Ignore external data and just load the model structure
                        without any weights. The model will be usable only for
                        purposes that don't require weights, such as
                        extracting subgraphs or inspecting model structure.
                        This can be useful in cases where external data is not
                        available.
  --fp-to-fp16          Convert all floating point tensors in an ONNX model to
                        16-bit precision. This is *not* needed in order to use
                        TensorRT's fp16 precision, but may be useful for other
                        backends. Requires onnxmltools.

Data Loader:
  Options related to loading or generating input data for inference.

  --seed SEED           Seed to use for random inputs
  --val-range VAL_RANGE [VAL_RANGE ...]
                        Range of values to generate in the data loader. To
                        specify per-input ranges, use the format: --val-range
                        <input_name>:[min,max]. If no input name is provided,
                        the range is used for any inputs not explicitly
                        specified. For example: --val-range [0,1] inp0:[2,50]
                        inp1:[3.0,4.6]
  --int-min INT_MIN     [DEPRECATED: Use --val-range] Minimum integer value
                        for random integer inputs
  --int-max INT_MAX     [DEPRECATED: Use --val-range] Maximum integer value
                        for random integer inputs
  --float-min FLOAT_MIN
                        [DEPRECATED: Use --val-range] Minimum float value for
                        random float inputs
  --float-max FLOAT_MAX
                        [DEPRECATED: Use --val-range] Maximum float value for
                        random float inputs
  --iterations NUM, --iters NUM
                        Number of inference iterations for which the default
                        data loader should supply data
  --data-loader-backend-module {numpy,torch}
                        The module to use for generating input arrays.
                        Currently supported options: numpy, torch
  --load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...], --load-input-data LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                        Path(s) to load inputs. The file(s) should be a JSON-
                        ified List[Dict[str, numpy.ndarray]], i.e. a list
                        where each element is the feed_dict for a single
                        iteration. When this option is used, all other data
                        loader arguments are ignored.
  --data-loader-script DATA_LOADER_SCRIPT
                        Path to a Python script that defines a function that
                        loads input data. The function should take no
                        arguments and return a generator or iterable that
                        yields input data (Dict[str, np.ndarray]). When this
                        option is used, all other data loader arguments are
                        ignored. By default, Polygraphy looks for a function
                        called `load_data`. You can specify a custom function
                        name by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --data-loader-func-name DATA_LOADER_FUNC_NAME
                        [DEPRECATED - function name can be specified with
                        --data-loader-script like so:
                        `my_custom_script.py:my_func`] When using a data-
                        loader-script, this specifies the name of the function
                        that loads data. Defaults to `load_data`.

TensorRT Builder Configuration:
  Options related to creating the TensorRT BuilderConfig.

  --trt-min-shapes TRT_MIN_SHAPES [TRT_MIN_SHAPES ...]
                        The minimum shapes the optimization profile(s) will
                        support. Specify this option once for each profile. If
                        not provided, inference-time input shapes are used.
                        Format: --trt-min-shapes <input0>:[D0,D1,..,DN] ..
                        <inputN>:[D0,D1,..,DN]
  --trt-opt-shapes TRT_OPT_SHAPES [TRT_OPT_SHAPES ...]
                        The shapes for which the optimization profile(s) will
                        be most performant. Specify this option once for each
                        profile. If not provided, inference-time input shapes
                        are used. Format: --trt-opt-shapes
                        <input0>:[D0,D1,..,DN] .. <inputN>:[D0,D1,..,DN]
  --trt-max-shapes TRT_MAX_SHAPES [TRT_MAX_SHAPES ...]
                        The maximum shapes the optimization profile(s) will
                        support. Specify this option once for each profile. If
                        not provided, inference-time input shapes are used.
                        Format: --trt-max-shapes <input0>:[D0,D1,..,DN] ..
                        <inputN>:[D0,D1,..,DN]
  --tf32                Enable tf32 precision in TensorRT
  --fp16                Enable fp16 precision in TensorRT
  --bf16                Enable bf16 precision in TensorRT
  --fp8                 Enable fp8 precision in TensorRT
  --int8                Enable int8 precision in TensorRT. If calibration is
                        required but no calibration cache is provided, this
                        option will cause TensorRT to run int8 calibration
                        using the Polygraphy data loader to provide
                        calibration data. If calibration is run and the model
                        has dynamic shapes, the last optimization profile will
                        be used as the calibration profile.
  --precision-constraints {prefer,obey,none}
                        If set to `prefer`, TensorRT will restrict available
                        tactics to layer precisions specified in the network
                        unless no implementation exists with the preferred
                        layer constraints, in which case it will issue a
                        warning and use the fastest available implementation.
                        If set to `obey`, TensorRT will instead fail to build
                        the network if no implementation exists with the
                        preferred layer constraints. Defaults to `obey`
  --sparse-weights      Enable optimizations for sparse weights in TensorRT
  --version-compatible  Builds an engine designed to be forward TensorRT
                        version compatible.
  --exclude-lean-runtime
                        Exclude the lean runtime from the plan when version
                        compatibility is enabled.
  --calibration-cache CALIBRATION_CACHE
                        Path to load/save a calibration cache. Used to store
                        calibration scales to speed up the process of int8
                        calibration. If the provided path does not yet exist,
                        int8 calibration scales will be calculated and written
                        to it during engine building. If the provided path
                        does exist, it will be read and int8 calibration will
                        be skipped during engine building.
  --calib-base-cls CALIBRATION_BASE_CLASS, --calibration-base-class CALIBRATION_BASE_CLASS
                        The name of the calibration base class to use. For
                        example, 'IInt8MinMaxCalibrator'.
  --quantile QUANTILE   The quantile to use for IInt8LegacyCalibrator. Has no
                        effect for other calibrator types.
  --regression-cutoff REGRESSION_CUTOFF
                        The regression cutoff to use for
                        IInt8LegacyCalibrator. Has no effect for other
                        calibrator types.
  --load-timing-cache LOAD_TIMING_CACHE
                        Path to load tactic timing cache. Used to cache tactic
                        timing information to speed up the engine building
                        process. If the file specified by --load-timing-cache
                        does not exist, Polygraphy will emit a warning and
                        fall back to using an empty timing cache.
  --error-on-timing-cache-miss
                        Emit error when a tactic being timed is not present in
                        the timing cache.
  --disable-compilation-cache
                        Disable caching JIT-compiled code
  --save-tactics SAVE_TACTICS, --save-tactic-replay SAVE_TACTICS
                        Path to save a Polygraphy tactic replay file. Details
                        about tactics selected by TensorRT will be recorded
                        and stored at this location as a JSON file.
  --load-tactics LOAD_TACTICS, --load-tactic-replay LOAD_TACTICS
                        Path to load a Polygraphy tactic replay file, such as
                        one created by --save-tactics. The tactics specified
                        in the file will be used to override TensorRT's
                        default selections.
  --tactic-sources [TACTIC_SOURCES ...]
                        Tactic sources to enable. This controls which
                        libraries (e.g. cudnn, cublas, etc.) TensorRT is
                        allowed to load tactics from. Values come from the
                        names of the values in the trt.TacticSource enum and
                        are case-insensitive. If no arguments are provided,
                        e.g. '--tactic-sources', then all tactic sources are
                        disabled.Defaults to TensorRT's default tactic
                        sources.
  --trt-config-script TRT_CONFIG_SCRIPT
                        Path to a Python script that defines a function that
                        creates a TensorRT IBuilderConfig. The function should
                        take a builder and network as parameters and return a
                        TensorRT builder configuration. When this option is
                        specified, all other config arguments are ignored. By
                        default, Polygraphy looks for a function called
                        `load_config`. You can specify a custom function name
                        by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --trt-config-func-name TRT_CONFIG_FUNC_NAME
                        [DEPRECATED - function name can be specified with
                        --trt-config-script like so:
                        `my_custom_script.py:my_func`]When using a trt-config-
                        script, this specifies the name of the function that
                        creates the config. Defaults to `load_config`.
  --trt-config-postprocess-script TRT_CONFIG_POSTPROCESS_SCRIPT, --trt-cpps TRT_CONFIG_POSTPROCESS_SCRIPT
                        [EXPERIMENTAL] Path to a Python script that defines a
                        function that modifies a TensorRT IBuilderConfig. This
                        function will be called after Polygraphy has finished
                        created the builder configuration and should take a
                        builder, network, and config as parameters and modify
                        the config in place. Unlike `--trt-config-script`, all
                        other config arguments will be reflected in the config
                        passed to the function.By default, Polygraphy looks
                        for a function called `postprocess_config`. You can
                        specify a custom function name by separating it with a
                        colon. For example: `my_custom_script.py:my_func`
  --trt-safety-restricted
                        Enable safety scope checking in TensorRT
  --refittable          Enable the engine to be refitted with new weights
                        after it is built.
  --strip-plan          Builds the engine with the refittable weights
                        stripped.
  --use-dla             [EXPERIMENTAL] Use DLA as the default device type
  --allow-gpu-fallback  [EXPERIMENTAL] Allow layers unsupported on the DLA to
                        fall back to GPU. Has no effect if --use-dla is not
                        set.
  --pool-limit MEMORY_POOL_LIMIT [MEMORY_POOL_LIMIT ...], --memory-pool-limit MEMORY_POOL_LIMIT [MEMORY_POOL_LIMIT ...]
                        Memory pool limits. Memory pool names come from the
                        names of values in the `trt.MemoryPoolType` enum and
                        are case-insensitiveFormat: `--pool-limit
                        <pool_name>:<pool_limit> ...`. For example, `--pool-
                        limit dla_local_dram:1e9 workspace:16777216`.
                        Optionally, use a `K`, `M`, or `G` suffix to indicate
                        KiB, MiB, or GiB respectively. For example, `--pool-
                        limit workspace:16M` is equivalent to `--pool-limit
                        workspace:16777216`.
  --preview-features [PREVIEW_FEATURES ...]
                        Preview features to enable. Values come from the names
                        of the values in the trt.PreviewFeature enum, and are
                        case-insensitive.If no arguments are provided, e.g. '
                        --preview-features', then all preview features are
                        disabled. Defaults to TensorRT's default preview
                        features.
  --builder-optimization-level BUILDER_OPTIMIZATION_LEVEL
                        The builder optimization level. Setting a higher
                        optimization level allows the optimizer to spend more
                        time searching for optimization opportunities. The
                        resulting engine may have better performance compared
                        to an engine built with a lower optimization level.
                        Refer to the TensorRT API documentation for details.
  --hardware-compatibility-level HARDWARE_COMPATIBILITY_LEVEL
                        The hardware compatibility level to use for the
                        engine. This allows engines built on one GPU
                        architecture to work on GPUs of other architectures.
                        Values come from the names of values in the
                        `trt.HardwareCompatibilityLevel` enum and are case-
                        insensitive. For example, `--hardware-compatibility-
                        level ampere_plus`
  --max-aux-streams MAX_AUX_STREAMS
                        The maximum number of auxiliary streams that TensorRT
                        is allowed to use. If the network contains operators
                        that can run in parallel, TRT can execute them using
                        auxiliary streams in addition to the one provided to
                        the IExecutionContext.execute_async_v3() call. The
                        default maximum number of auxiliary streams is
                        determined by the heuristics in TensorRT on whether
                        enabling multi-stream would improve the performance.
                        Refer to the TensorRT API documentation for details.
  --quantization-flags [QUANTIZATION_FLAGS ...]
                        Int8 quantization flags to enable. Values come from
                        the names of values in the trt.QuantizationFlag enum,
                        and are case-insensitive. If no arguments are
                        provided, e.g. '--quantization-flags', then all
                        quantization flags are disabled. Defaults to
                        TensorRT's default quantization flags.
  --profiling-verbosity PROFILING_VERBOSITY
                        The verbosity of NVTX annotations in the generated
                        engine.Values come from the names of values in the
                        `trt.ProfilingVerbosity` enum and are case-
                        insensitive. For example, `--profiling-verbosity
                        detailed`. Defaults to 'verbose'.
  --weight-streaming    Build a weight streamable engine. Must be set with
                        --strongly-typed. The weight streaming amount can be
                        set with --weight-streaming-budget.
  --runtime-platform RUNTIME_PLATFORM
                        The target runtime platform (operating system and CPU
                        architecture) for the execution of the TensorRT
                        engine. TensorRT provides support for cross-platform
                        engine compatibility when the target runtime platform
                        is different from the build platform. Values come from
                        the names of values in the `trt.RuntimePlatform` enum
                        and are case-insensitive. For example, `--runtime-
                        platform same_as_build`, `--runtime-platform
                        windows_amd64`

TensorRT Plugin Loading:
  Options related to loading TensorRT plugins.

  --plugins PLUGINS [PLUGINS ...]
                        Path(s) of plugin libraries to load

ONNX-TRT Parser Flags:
  Options related to setting flags for TensorRT's ONNX parser

  --onnx-flags ONNX_FLAGS [ONNX_FLAGS ...]
                        Flag(s) for adjusting the default parsing behavior of
                        the ONNX parser.Flag values come from the
                        `trt.OnnxParserFlag` enum and are case-insensitve.For
                        example: --onnx-flags native_instancenorm
  --plugin-instancenorm
                        Switch to clear the
                        `trt.OnnxParserFlag.NATIVE_INSTANCENORM` flag andforce
                        the usage of the plugin implementation of ONNX
                        InstanceNorm.Note that
                        `trt.OnnxParserFlag.NATIVE_INSTANCENORM` is ON by
                        default since TensorRT 10.0.

TensorRT Network Loading:
  Options related to loading TensorRT networks.

  --trt-outputs TRT_OUTPUTS [TRT_OUTPUTS ...]
                        Name(s) of TensorRT output(s). Using '--trt-outputs
                        mark all' indicates that all tensors should be used as
                        outputs
  --trt-exclude-outputs TRT_EXCLUDE_OUTPUTS [TRT_EXCLUDE_OUTPUTS ...]
                        [EXPERIMENTAL] Name(s) of TensorRT output(s) to unmark
                        as outputs.
  --layer-precisions LAYER_PRECISIONS [LAYER_PRECISIONS ...]
                        Compute precision to use for each layer. This should
                        be specified on a per-layer basis, using the format:
                        --layer-precisions <layer_name>:<layer_precision>.
                        Precision values come from the TensorRT data type
                        aliases, like float32, float16, int8, bool, etc. For
                        example: --layer-precisions example_layer:float16
                        other_layer:int8. When this option is provided, you
                        should also set --precision-constraints to either
                        'prefer' or 'obey'.
  --tensor-dtypes TENSOR_DTYPES [TENSOR_DTYPES ...], --tensor-datatypes TENSOR_DTYPES [TENSOR_DTYPES ...]
                        Data type to use for each network I/O tensor. This
                        should be specified on a per-tensor basis, using the
                        format: --tensor-datatypes
                        <tensor_name>:<tensor_datatype>. Data type values come
                        from the TensorRT data type aliases, like float32,
                        float16, int8, bool, etc. For example: --tensor-
                        datatypes example_tensor:float16 other_tensor:int8.
  --trt-network-func-name TRT_NETWORK_FUNC_NAME
                        [DEPRECATED - function name can be specified alongside
                        the script like so: `my_custom_script.py:my_func`]
                        When using a trt-network-script instead of other model
                        types, this specifies the name of the function that
                        loads the network. Defaults to `load_network`.
  --trt-network-postprocess-script TRT_NETWORK_POSTPROCESS_SCRIPT [TRT_NETWORK_POSTPROCESS_SCRIPT ...], --trt-npps TRT_NETWORK_POSTPROCESS_SCRIPT [TRT_NETWORK_POSTPROCESS_SCRIPT ...]
                        [EXPERIMENTAL] Specify a post-processing script to run
                        on the parsed TensorRT network. The script file may
                        optionally be suffixed with the name of the callable
                        to be invoked. For example: `--trt-npps
                        process.py:do_something`. If no callable is specified,
                        then by default Polygraphy uses the callable name
                        `postprocess`. The callable is expected to take a
                        named argument `network` of type
                        `trt.INetworkDefinition`. Multiple scripts may be
                        specified, in which case they are executed in the
                        order given.
  --strongly-typed      Mark the network as being strongly typed.
  --mark-debug MARK_DEBUG [MARK_DEBUG ...]
                        Specify list of names of tensors to be marked as debug
                        tensors.For example, `--mark-debug tensor1 tensor2
                        tensor3`.

TensorRT Engine:
  Options related to loading or building TensorRT engines.

  --save-timing-cache SAVE_TIMING_CACHE
                        Path to save tactic timing cache if building an
                        engine. Existing caches will be appended to with any
                        new timing information gathered.
