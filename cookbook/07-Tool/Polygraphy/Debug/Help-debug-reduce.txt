usage: polygraphy debug reduce [-h] [-v] [-q]
                               [--verbosity VERBOSITY [VERBOSITY ...]]
                               [--silent]
                               [--log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]]
                               [--log-file LOG_FILE] [--check ...]
                               [--fail-code FAIL_CODES [FAIL_CODES ...] |
                               --ignore-fail-code IGNORE_FAIL_CODES
                               [IGNORE_FAIL_CODES ...]]
                               [--fail-regex FAIL_REGEX [FAIL_REGEX ...]]
                               [--show-output | --hide-fail-output]
                               [--artifacts ARTIFACTS [ARTIFACTS ...]]
                               [--art-dir DIR]
                               [--iter-artifact ITER_ARTIFACT_PATH]
                               [--no-remove-intermediate]
                               [--iter-info ITERATION_INFO_PATH]
                               [--load-debug-replay LOAD_DEBUG_REPLAY]
                               [--save-debug-replay SAVE_DEBUG_REPLAY]
                               [--model-input-shapes INPUT_SHAPES [INPUT_SHAPES ...]]
                               [-o SAVE_ONNX]
                               [--save-external-data [EXTERNAL_DATA_PATH]]
                               [--external-data-size-threshold EXTERNAL_DATA_SIZE_THRESHOLD]
                               [--no-save-all-tensors-to-one-file]
                               [--no-shape-inference | --force-fallback-shape-inference]
                               [--no-onnxruntime-shape-inference]
                               [--external-data-dir EXTERNAL_DATA_DIR]
                               [--ignore-external-data] [--fp-to-fp16]
                               [--seed SEED]
                               [--val-range VAL_RANGE [VAL_RANGE ...]]
                               [--int-min INT_MIN] [--int-max INT_MAX]
                               [--float-min FLOAT_MIN] [--float-max FLOAT_MAX]
                               [--iterations NUM]
                               [--data-loader-backend-module {numpy,torch}]
                               [--load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                               | --data-loader-script DATA_LOADER_SCRIPT]
                               [--data-loader-func-name DATA_LOADER_FUNC_NAME]
                               [--min-good MIN_GOOD]
                               [--no-reduce-inputs | --no-reduce-outputs]
                               [--mode {bisect,linear}]
                               model_file

[EXPERIMENTAL] Reduce a failing ONNX model to the minimum set of nodes that cause the failure.

`debug reduce` follows the same general process as other `debug` subtools (refer to the help output
of the `debug` tool for more background information and details).

Specifically, it does the following during each iteration:

1. Generates a successively smaller subgraph of a given ONNX model and saves it in the
    current directory as `polygraphy_debug.onnx` by default.

2. Evaluates it using one of two methods:
    a. In an automated fashion, if a `--check` command was provided.
    b. In an interactive fashion otherwise. In interactive mode, the tool will prompt you to report whether
        the iteration passed or failed.
   In either case, if the iteration fails, it further reduces the model during the subsequent iteration.
   Otherwise, it expands the model to include more nodes from the original.

3. When the model cannot be reduced further, it saves it to the path specfied by `--output`.

4. Optionally, as with other `debug` subtools, it can track and sort additional files specified by `--artifacts`.

NOTE: When your model includes dynamic input shapes, it is generally a good idea to tell `debug reduce` what
    shapes to use with the `--model-input-shapes` argument. Further, if your model uses shape operations,
    you should freeze the input shapes and then fold the shape operations prior to running `debug reduce`:
        `polygraphy surgeon sanitize --fold-constants --override-input-shapes <static_input_shapes>`

The typical usage of `debug reduce` is:

    polygraphy debug reduce <onnx_model> --output <reduced_model> \
        [--check <check_command>]

`polygraphy run` is usually a good choice for the `--check` command.

options:
  -h, --help            show this help message and exit
  --min-good MIN_GOOD, --minimal-good MIN_GOOD
                        Path at which to save an ONNX model close in size to
                        the reduced model that does not have the failure. This
                        is not guaranteed to be generated.
  --no-reduce-inputs    Do not attempt to change the graph inputs to reduce
                        the model further. 'reduce' will then only attempt to
                        find the earliest failing outputs.
  --no-reduce-outputs   Do not attempt to change the graph outputs to reduce
                        the model further. 'reduce' will then only attempt to
                        find the latest failing inputs.
  --mode {bisect,linear}
                        Strategy to use to iteratively remove nodes from the
                        model. 'bisect' will use binary search, and 'linear'
                        will delete one node at a time. 'linear' mode may be
                        significantly slower, but can offer better results in
                        models with branches. One strategy is to use 'bisect'
                        first, and then further reduce the result with
                        'linear'. Defaults to 'bisect'.

Logging:
  Options related to logging and debug output

  -v, --verbose         Increase logging verbosity. Specify multiple times for
                        higher verbosity
  -q, --quiet           Decrease logging verbosity. Specify multiple times for
                        lower verbosity
  --verbosity VERBOSITY [VERBOSITY ...]
                        The logging verbosity to use. Takes precedence over
                        the `-v` and `-q` options, and unlike them, allows you
                        to control per-path verbosity. Verbosity values should
                        come from Polygraphy's logging verbosities defined in
                        the `Logger` class and are case-insensitive. For
                        example: `--verbosity INFO` or `--verbosity verbose`.
                        To specify per-path verbosity, use the format:
                        `<path>:<verbosity>`. For example: `--verbosity
                        backend/trt:INFO backend/trt/loader.py:VERBOSE`. Paths
                        should be relative to the `polygraphy/` directory. For
                        example, `polygraphy/backend` should be specified with
                        just `backend`. The most closely matching path is used
                        to determine verbosity. For example, with:
                        `--verbosity warning backend:info
                        backend/trt:verbose`, a file under
                        `polygraphy/comparator` would use `WARNING` verbosity,
                        one under `backend/onnx` would use `INFO`, and one
                        under `backend/trt` would use `VERBOSE`.
  --silent              Disable all output
  --log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]
                        Format for log messages: {{'timestamp': Include
                        timestamp, 'line-info': Include file and line number,
                        'no-colors': Disable colors}}
  --log-file LOG_FILE   Path to a file where Polygraphy logging output should
                        be written. This may not include logging output from
                        dependencies, like TensorRT or ONNX-Runtime.

Pass/Fail Reporting:
  Options related to reporting pass/fail status during iterative debugging.

  --check ..., --check-inference ...
                        A command to check the model. When this is omitted, an
                        interactive debugging session is started instead.By
                        default an exit status of 0 is treated as a 'pass'
                        whereas any other exit status is treated as a 'fail'.
  --fail-code FAIL_CODES [FAIL_CODES ...], --fail-returncode FAIL_CODES [FAIL_CODES ...]
                        The return code(s) from the --check command to count
                        as failures. If this is provided, any other return
                        code will be counted as a success.
  --ignore-fail-code IGNORE_FAIL_CODES [IGNORE_FAIL_CODES ...], --ignore-fail-returncode IGNORE_FAIL_CODES [IGNORE_FAIL_CODES ...]
                        The return code(s) from the --check command to ignore
                        as failures.
  --fail-regex FAIL_REGEX [FAIL_REGEX ...]
                        Regular expression denoting an error in the check
                        command's output. The command is only considered a
                        failure if a matching string is found in the command's
                        output. This can be useful to distinguish among
                        multiple types of failures. Can be specified multiple
                        times to match different regular expressions, in which
                        case any match counts as a failure. When combined with
                        --fail-code, only iterations whose return code is
                        considered a failure are checked for regular
                        expressions.
  --show-output         Show output from the --check command even for passing
                        iterations. By default, output from passing iterations
                        is captured.
  --hide-fail-output    Suppress output from the --check command for failing
                        iterations. By default, output from failing iterations
                        is displayed.

Artifact Sorting:
  Options related to sorting artifacts into good/bad directories based on pass/fail status.

  --artifacts ARTIFACTS [ARTIFACTS ...]
                        Path(s) of artifacts to sort. These will be moved into
                        'good' and 'bad' directories based on the exit status
                        of the `--check` command and suffixed with an
                        iteration number, timestamp and return code.
  --art-dir DIR, --artifacts-dir DIR
                        The directory in which to move artifacts and sort them
                        into 'good' and 'bad'. Defaults to a directory named
                        `polygraphy_artifacts` in the current directory.

Iterative Debugging:
  Options related to iteratively debugging.

  --iter-artifact ITER_ARTIFACT_PATH, --intermediate-artifact ITER_ARTIFACT_PATH
                        Path to store the intermediate artifact from each
                        iteration. Defaults to: polygraphy_debug.onnx
  --no-remove-intermediate
                        Do not remove the intermediate artifact between
                        iterations. Subsequent iterations may still overwrite
                        the artifact from previous iterations. This allows you
                        to exit the tool early and still have access to the
                        most recent intermediate artifact.
  --iter-info ITERATION_INFO_PATH, --iteration-info ITERATION_INFO_PATH
                        Path to write a JSON file containing information about
                        the current iteration. This will include an
                        'iteration' key whose value is the current iteration
                        number.
  --load-debug-replay LOAD_DEBUG_REPLAY
                        Path from which to load a debug replay. A replay file
                        includes information on the results of some or all
                        iterations, allowing you to skip those iterations.
  --save-debug-replay SAVE_DEBUG_REPLAY
                        Path at which to save a debug replay, which includes
                        information on the results of debugging iterations.
                        The replay can be used with `--load-debug-replay` to
                        skip iterations during subsequent debugging sessions.
                        The replay is saved after the first iteration and
                        overwritten with an updated replay during each
                        iteration thereafter. This will also write a second
                        replay file with a suffix of `_skip_current`, which is
                        written before the iteration completes, and treats it
                        as a failure. In cases where the iteration crashes,
                        loading this replay file provides a means of skipping
                        over the crash. Defaults to
                        `polygraphy_debug_replay.json` in the current
                        directory.

Model:
  Options related to the model

  model_file            Path to the model
  --model-input-shapes INPUT_SHAPES [INPUT_SHAPES ...], --model-inputs INPUT_SHAPES [INPUT_SHAPES ...]
                        Model input(s) and their shape(s). Used to determine
                        shapes to use while generating input data for
                        inference. Format: --model-input-shapes
                        <name>:<shape>. For example: --model-input-shapes
                        image:[1,3,224,224] other_input:[10]

ONNX Model Saving:
  Options related to saving ONNX models.

  -o SAVE_ONNX, --output SAVE_ONNX
                        Path to save the ONNX model
  --save-external-data [EXTERNAL_DATA_PATH], --external-data-path [EXTERNAL_DATA_PATH]
                        Whether to save weight data in external file(s). To
                        use a non-default path, supply the desired path as an
                        argument. This is always a relative path; external
                        data is always written to the same directory as the
                        model.
  --external-data-size-threshold EXTERNAL_DATA_SIZE_THRESHOLD
                        The size threshold, in bytes, above which tensor data
                        will be stored in the external file. Tensors smaller
                        that this threshold will remain in the ONNX file.
                        Optionally, use a `K`, `M`, or `G` suffix to indicate
                        KiB, MiB, or GiB respectively. For example,
                        `--external-data-size-threshold=16M` is equivalent to
                        `--external-data-size-threshold=16777216`. Has no
                        effect if `--save-external-data` is not set. Defaults
                        to 1024 bytes.
  --no-save-all-tensors-to-one-file
                        Do not save all tensors to a single file when saving
                        external data. Has no effect if `--save-external-data`
                        is not set

ONNX Shape Inference:
  Options related to ONNX shape inference.

  --no-shape-inference  Disable ONNX shape inference when loading the model
  --force-fallback-shape-inference
                        Force Polygraphy to use ONNX-Runtime to determine
                        metadata for tensors in the graph. This can be useful
                        in cases where ONNX shape inference does not generate
                        correct information. Note that this will cause dynamic
                        dimensions to become static.
  --no-onnxruntime-shape-inference
                        Disable using ONNX-Runtime's shape inference
                        utilities. This will force Polygraphy to use
                        `onnx.shape_inference` instead. Note that ONNX-
                        Runtime's shape inference utilities may be more
                        performant and memory-efficient.

ONNX Model Loading:
  Options related to loading ONNX models.

  --external-data-dir EXTERNAL_DATA_DIR, --load-external-data EXTERNAL_DATA_DIR, --ext EXTERNAL_DATA_DIR
                        Path to a directory containing external data for the
                        model. Generally, this is only required if the
                        external data is not stored in the model directory.
  --ignore-external-data
                        Ignore external data and just load the model structure
                        without any weights. The model will be usable only for
                        purposes that don't require weights, such as
                        extracting subgraphs or inspecting model structure.
                        This can be useful in cases where external data is not
                        available.
  --fp-to-fp16          Convert all floating point tensors in an ONNX model to
                        16-bit precision. This is *not* needed in order to use
                        TensorRT's fp16 precision, but may be useful for other
                        backends. Requires onnxmltools.

Data Loader:
  Options related to loading or generating input data for inference.

  --seed SEED           Seed to use for random inputs
  --val-range VAL_RANGE [VAL_RANGE ...]
                        Range of values to generate in the data loader. To
                        specify per-input ranges, use the format: --val-range
                        <input_name>:[min,max]. If no input name is provided,
                        the range is used for any inputs not explicitly
                        specified. For example: --val-range [0,1] inp0:[2,50]
                        inp1:[3.0,4.6]
  --int-min INT_MIN     [DEPRECATED: Use --val-range] Minimum integer value
                        for random integer inputs
  --int-max INT_MAX     [DEPRECATED: Use --val-range] Maximum integer value
                        for random integer inputs
  --float-min FLOAT_MIN
                        [DEPRECATED: Use --val-range] Minimum float value for
                        random float inputs
  --float-max FLOAT_MAX
                        [DEPRECATED: Use --val-range] Maximum float value for
                        random float inputs
  --iterations NUM, --iters NUM
                        Number of inference iterations for which the default
                        data loader should supply data
  --data-loader-backend-module {numpy,torch}
                        The module to use for generating input arrays.
                        Currently supported options: numpy, torch
  --load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...], --load-input-data LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                        Path(s) to load inputs. The file(s) should be a JSON-
                        ified List[Dict[str, numpy.ndarray]], i.e. a list
                        where each element is the feed_dict for a single
                        iteration. When this option is used, all other data
                        loader arguments are ignored.
  --data-loader-script DATA_LOADER_SCRIPT
                        Path to a Python script that defines a function that
                        loads input data. The function should take no
                        arguments and return a generator or iterable that
                        yields input data (Dict[str, np.ndarray]). When this
                        option is used, all other data loader arguments are
                        ignored. By default, Polygraphy looks for a function
                        called `load_data`. You can specify a custom function
                        name by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --data-loader-func-name DATA_LOADER_FUNC_NAME
                        [DEPRECATED - function name can be specified with
                        --data-loader-script like so:
                        `my_custom_script.py:my_func`] When using a data-
                        loader-script, this specifies the name of the function
                        that loads data. Defaults to `load_data`.
