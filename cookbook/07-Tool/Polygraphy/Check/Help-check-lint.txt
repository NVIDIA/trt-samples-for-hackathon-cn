usage: polygraphy check lint [-h] [-v] [-q]
                             [--verbosity VERBOSITY [VERBOSITY ...]]
                             [--silent]
                             [--log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]]
                             [--log-file LOG_FILE]
                             [--input-shapes INPUT_SHAPES [INPUT_SHAPES ...]]
                             [--external-data-dir EXTERNAL_DATA_DIR]
                             [--ignore-external-data] [--fp-to-fp16]
                             [--seed SEED]
                             [--val-range VAL_RANGE [VAL_RANGE ...]]
                             [--int-min INT_MIN] [--int-max INT_MAX]
                             [--float-min FLOAT_MIN] [--float-max FLOAT_MAX]
                             [--iterations NUM]
                             [--data-loader-backend-module {numpy,torch}]
                             [--load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                             | --data-loader-script DATA_LOADER_SCRIPT]
                             [--data-loader-func-name DATA_LOADER_FUNC_NAME]
                             [--providers PROVIDERS [PROVIDERS ...]]
                             [-o OUTPUT]
                             model_file

[EXPERIMENTAL] Topologically "lint" an ONNX model to find faulty nodes in the graph.
All nodes that depend on a faulty node will be marked as faulty and ignored.

All error messages and warnings are captured in a JSON report.

The JSON report contains the following fields:
- 'summary' : summarizes the passing and failing nodes among the ones that are linted.
(Note: the nodes included are not exhaustive, as some nodes may be skipped due to dependency on a faulty previous node)
- 'linting_entries': a list of linting entries, each of which contains the following fields:
    - 'level': the severity of the linting entry (error or warning)
    - 'source': The underlying checker that generated the error message (either `onnx.checker` or ONNX Runtime)
    - 'message': The error message. This message is superficially parsed/pruned but may retain formatting of the underlying checker.
    - (optional) 'nodes': A list of nodes that are related to the error message. If this field is not present,
        then the linting entry is a global error/warning that applies to the entire model (like a missing opset import).

The schema for the json output is:
    {
        'summary': {
            'passing': [<list of nodes that passed ORT inference check>],
            'failing': [<list of nodes that failed ORT inference check>],
            },
        'lint_entries': [
            { 'level': <severity level>, 'source': <source of error>, 'message': <error string>, 'nodes': [<name of failing node>] },
            ...
        ]
    }

Known Limitations:
------------------
1. BFLOAT16 and FLOAT8  are not currently supported.
2. Only erroneous nodes that are independent of each other are captured in the JSON report. Downstream nodes that depend on a faulty node are not checked.
3. Subgraph nested inside nodes are not recursively linted.
4. Custom Ops are documented as warnings in the JSON Report, but are treated as exceptions by the internal inference checks. Therefore downstream nodes that depend on the custom op are not checked for error or custom ops.
5. The subtool verifies data-dependent failures either based on user's input data or generating random data for the input tensors. Therefore, the subtool's coverage of subgraphs are completely dependent on the input data and does not guarantee 100% coverage.
For example, if a subgraph has a conditional branch, the subtool will only check the branch that is taken based on the input data.
6. Large models (>2GB) require external data to be in same directory as the model file, custom paths to external data are not supported.

options:
  -h, --help            show this help message and exit
  -o OUTPUT, --output OUTPUT
                        Path to save json report.

Logging:
  Options related to logging and debug output

  -v, --verbose         Increase logging verbosity. Specify multiple times for
                        higher verbosity
  -q, --quiet           Decrease logging verbosity. Specify multiple times for
                        lower verbosity
  --verbosity VERBOSITY [VERBOSITY ...]
                        The logging verbosity to use. Takes precedence over
                        the `-v` and `-q` options, and unlike them, allows you
                        to control per-path verbosity. Verbosity values should
                        come from Polygraphy's logging verbosities defined in
                        the `Logger` class and are case-insensitive. For
                        example: `--verbosity INFO` or `--verbosity verbose`.
                        To specify per-path verbosity, use the format:
                        `<path>:<verbosity>`. For example: `--verbosity
                        backend/trt:INFO backend/trt/loader.py:VERBOSE`. Paths
                        should be relative to the `polygraphy/` directory. For
                        example, `polygraphy/backend` should be specified with
                        just `backend`. The most closely matching path is used
                        to determine verbosity. For example, with:
                        `--verbosity warning backend:info
                        backend/trt:verbose`, a file under
                        `polygraphy/comparator` would use `WARNING` verbosity,
                        one under `backend/onnx` would use `INFO`, and one
                        under `backend/trt` would use `VERBOSE`.
  --silent              Disable all output
  --log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]
                        Format for log messages: {{'timestamp': Include
                        timestamp, 'line-info': Include file and line number,
                        'no-colors': Disable colors}}
  --log-file LOG_FILE   Path to a file where Polygraphy logging output should
                        be written. This may not include logging output from
                        dependencies, like TensorRT or ONNX-Runtime.

Model:
  Options related to the model

  model_file            Path to the model
  --input-shapes INPUT_SHAPES [INPUT_SHAPES ...], --inputs INPUT_SHAPES [INPUT_SHAPES ...]
                        Model input(s) and their shape(s). Used to determine
                        shapes to use while generating input data for
                        inference. Format: --input-shapes <name>:<shape>. For
                        example: --input-shapes image:[1,3,224,224]
                        other_input:[10]

ONNX Model Loading:
  Options related to loading ONNX models.

  --external-data-dir EXTERNAL_DATA_DIR, --load-external-data EXTERNAL_DATA_DIR, --ext EXTERNAL_DATA_DIR
                        Path to a directory containing external data for the
                        model. Generally, this is only required if the
                        external data is not stored in the model directory.
  --ignore-external-data
                        Ignore external data and just load the model structure
                        without any weights. The model will be usable only for
                        purposes that don't require weights, such as
                        extracting subgraphs or inspecting model structure.
                        This can be useful in cases where external data is not
                        available.
  --fp-to-fp16          Convert all floating point tensors in an ONNX model to
                        16-bit precision. This is *not* needed in order to use
                        TensorRT's fp16 precision, but may be useful for other
                        backends. Requires onnxmltools.

Data Loader:
  Options related to loading or generating input data for inference.

  --seed SEED           Seed to use for random inputs
  --val-range VAL_RANGE [VAL_RANGE ...]
                        Range of values to generate in the data loader. To
                        specify per-input ranges, use the format: --val-range
                        <input_name>:[min,max]. If no input name is provided,
                        the range is used for any inputs not explicitly
                        specified. For example: --val-range [0,1] inp0:[2,50]
                        inp1:[3.0,4.6]
  --int-min INT_MIN     [DEPRECATED: Use --val-range] Minimum integer value
                        for random integer inputs
  --int-max INT_MAX     [DEPRECATED: Use --val-range] Maximum integer value
                        for random integer inputs
  --float-min FLOAT_MIN
                        [DEPRECATED: Use --val-range] Minimum float value for
                        random float inputs
  --float-max FLOAT_MAX
                        [DEPRECATED: Use --val-range] Maximum float value for
                        random float inputs
  --iterations NUM, --iters NUM
                        Number of inference iterations for which the default
                        data loader should supply data
  --data-loader-backend-module {numpy,torch}
                        The module to use for generating input arrays.
                        Currently supported options: numpy, torch
  --load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...], --load-input-data LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                        Path(s) to load inputs. The file(s) should be a JSON-
                        ified List[Dict[str, numpy.ndarray]], i.e. a list
                        where each element is the feed_dict for a single
                        iteration. When this option is used, all other data
                        loader arguments are ignored.
  --data-loader-script DATA_LOADER_SCRIPT
                        Path to a Python script that defines a function that
                        loads input data. The function should take no
                        arguments and return a generator or iterable that
                        yields input data (Dict[str, np.ndarray]). When this
                        option is used, all other data loader arguments are
                        ignored. By default, Polygraphy looks for a function
                        called `load_data`. You can specify a custom function
                        name by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --data-loader-func-name DATA_LOADER_FUNC_NAME
                        [DEPRECATED - function name can be specified with
                        --data-loader-script like so:
                        `my_custom_script.py:my_func`] When using a data-
                        loader-script, this specifies the name of the function
                        that loads data. Defaults to `load_data`.

ONNX-Runtime Session Creation:
  Options related to creating an ONNX-Runtime Inference Session

  --providers PROVIDERS [PROVIDERS ...], --execution-providers PROVIDERS [PROVIDERS ...]
                        A list of execution providers to use in order of
                        priority. Each provider may be either an exact match
                        or a case-insensitive partial match for the execution
                        providers available in ONNX-Runtime. For example, a
                        value of 'cpu' would match the 'CPUExecutionProvider'
