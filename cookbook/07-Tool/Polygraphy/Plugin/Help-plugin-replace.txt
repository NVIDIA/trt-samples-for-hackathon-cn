usage: polygraphy plugin replace [-h] [-v] [-q]
                                 [--verbosity VERBOSITY [VERBOSITY ...]]
                                 [--silent]
                                 [--log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]]
                                 [--log-file LOG_FILE]
                                 [--model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}]
                                 [--input-shapes INPUT_SHAPES [INPUT_SHAPES ...]]
                                 [--shape-inference]
                                 [--no-onnxruntime-shape-inference]
                                 [--external-data-dir EXTERNAL_DATA_DIR]
                                 [--ignore-external-data]
                                 [--onnx-outputs ONNX_OUTPUTS [ONNX_OUTPUTS ...]]
                                 [--onnx-exclude-outputs ONNX_EXCLUDE_OUTPUTS [ONNX_EXCLUDE_OUTPUTS ...]]
                                 [--fp-to-fp16] [--seed SEED]
                                 [--val-range VAL_RANGE [VAL_RANGE ...]]
                                 [--int-min INT_MIN] [--int-max INT_MAX]
                                 [--float-min FLOAT_MIN]
                                 [--float-max FLOAT_MAX] [--iterations NUM]
                                 [--data-loader-backend-module {numpy,torch}]
                                 [--load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                                 | --data-loader-script DATA_LOADER_SCRIPT]
                                 [--data-loader-func-name DATA_LOADER_FUNC_NAME]
                                 --plugin-dir PLUGIN_DIR -o OUTPUT
                                 [--config CONFIG]
                                 model_file

Replace a subgraph in an onnx model with a plugin.

options:
  -h, --help            show this help message and exit
  --plugin-dir PLUGIN_DIR
                        Plugin directory.
  -o OUTPUT, --output OUTPUT
                        Where to save the modified model
  --config CONFIG       location of config.yaml.

Logging:
  Options related to logging and debug output

  -v, --verbose         Increase logging verbosity. Specify multiple times for
                        higher verbosity
  -q, --quiet           Decrease logging verbosity. Specify multiple times for
                        lower verbosity
  --verbosity VERBOSITY [VERBOSITY ...]
                        The logging verbosity to use. Takes precedence over
                        the `-v` and `-q` options, and unlike them, allows you
                        to control per-path verbosity. Verbosity values should
                        come from Polygraphy's logging verbosities defined in
                        the `Logger` class and are case-insensitive. For
                        example: `--verbosity INFO` or `--verbosity verbose`.
                        To specify per-path verbosity, use the format:
                        `<path>:<verbosity>`. For example: `--verbosity
                        backend/trt:INFO backend/trt/loader.py:VERBOSE`. Paths
                        should be relative to the `polygraphy/` directory. For
                        example, `polygraphy/backend` should be specified with
                        just `backend`. The most closely matching path is used
                        to determine verbosity. For example, with:
                        `--verbosity warning backend:info
                        backend/trt:verbose`, a file under
                        `polygraphy/comparator` would use `WARNING` verbosity,
                        one under `backend/onnx` would use `INFO`, and one
                        under `backend/trt` would use `VERBOSE`.
  --silent              Disable all output
  --log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]
                        Format for log messages: {{'timestamp': Include
                        timestamp, 'line-info': Include file and line number,
                        'no-colors': Disable colors}}
  --log-file LOG_FILE   Path to a file where Polygraphy logging output should
                        be written. This may not include logging output from
                        dependencies, like TensorRT or ONNX-Runtime.

Model:
  Options related to the model

  model_file            Path to the model
  --model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}
                        The type of the input model: {{'frozen': TensorFlow
                        frozen graph; 'keras': Keras model; 'ckpt': TensorFlow
                        checkpoint directory; 'onnx': ONNX model; 'engine':
                        TensorRT engine; 'trt-network-script': A Python script
                        that defines a `load_network` function that takes no
                        arguments and returns a TensorRT Builder, Network, and
                        optionally Parser. If the function name is not
                        `load_network`, it can be specified after the model
                        file, separated by a colon. For example:
                        `my_custom_script.py:my_func`; 'uff': UFF file
                        [deprecated]; 'caffe': Caffe prototxt [deprecated]}}
  --input-shapes INPUT_SHAPES [INPUT_SHAPES ...], --inputs INPUT_SHAPES [INPUT_SHAPES ...]
                        Model input(s) and their shape(s). Used to determine
                        shapes to use while generating input data for
                        inference. Format: --input-shapes <name>:<shape>. For
                        example: --input-shapes image:[1,3,224,224]
                        other_input:[10]

ONNX Shape Inference:
  Options related to ONNX shape inference.

  --shape-inference, --do-shape-inference
                        Enable ONNX shape inference when loading the model
  --no-onnxruntime-shape-inference
                        Disable using ONNX-Runtime's shape inference
                        utilities. This will force Polygraphy to use
                        `onnx.shape_inference` instead. Note that ONNX-
                        Runtime's shape inference utilities may be more
                        performant and memory-efficient.

ONNX Model Loading:
  Options related to loading ONNX models.

  --external-data-dir EXTERNAL_DATA_DIR, --load-external-data EXTERNAL_DATA_DIR, --ext EXTERNAL_DATA_DIR
                        Path to a directory containing external data for the
                        model. Generally, this is only required if the
                        external data is not stored in the model directory.
  --ignore-external-data
                        Ignore external data and just load the model structure
                        without any weights. The model will be usable only for
                        purposes that don't require weights, such as
                        extracting subgraphs or inspecting model structure.
                        This can be useful in cases where external data is not
                        available.
  --onnx-outputs ONNX_OUTPUTS [ONNX_OUTPUTS ...]
                        Name(s) of ONNX tensor(s) to mark as output(s). Using
                        the special value 'mark all' indicates that all
                        tensors should be used as outputs
  --onnx-exclude-outputs ONNX_EXCLUDE_OUTPUTS [ONNX_EXCLUDE_OUTPUTS ...]
                        [EXPERIMENTAL] Name(s) of ONNX output(s) to unmark as
                        outputs.
  --fp-to-fp16          Convert all floating point tensors in an ONNX model to
                        16-bit precision. This is *not* needed in order to use
                        TensorRT's fp16 precision, but may be useful for other
                        backends. Requires onnxmltools.

Data Loader:
  Options related to loading or generating input data for inference.

  --seed SEED           Seed to use for random inputs
  --val-range VAL_RANGE [VAL_RANGE ...]
                        Range of values to generate in the data loader. To
                        specify per-input ranges, use the format: --val-range
                        <input_name>:[min,max]. If no input name is provided,
                        the range is used for any inputs not explicitly
                        specified. For example: --val-range [0,1] inp0:[2,50]
                        inp1:[3.0,4.6]
  --int-min INT_MIN     [DEPRECATED: Use --val-range] Minimum integer value
                        for random integer inputs
  --int-max INT_MAX     [DEPRECATED: Use --val-range] Maximum integer value
                        for random integer inputs
  --float-min FLOAT_MIN
                        [DEPRECATED: Use --val-range] Minimum float value for
                        random float inputs
  --float-max FLOAT_MAX
                        [DEPRECATED: Use --val-range] Maximum float value for
                        random float inputs
  --iterations NUM, --iters NUM
                        Number of inference iterations for which the default
                        data loader should supply data
  --data-loader-backend-module {numpy,torch}
                        The module to use for generating input arrays.
                        Currently supported options: numpy, torch
  --load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...], --load-input-data LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                        Path(s) to load inputs. The file(s) should be a JSON-
                        ified List[Dict[str, numpy.ndarray]], i.e. a list
                        where each element is the feed_dict for a single
                        iteration. When this option is used, all other data
                        loader arguments are ignored.
  --data-loader-script DATA_LOADER_SCRIPT
                        Path to a Python script that defines a function that
                        loads input data. The function should take no
                        arguments and return a generator or iterable that
                        yields input data (Dict[str, np.ndarray]). When this
                        option is used, all other data loader arguments are
                        ignored. By default, Polygraphy looks for a function
                        called `load_data`. You can specify a custom function
                        name by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --data-loader-func-name DATA_LOADER_FUNC_NAME
                        [DEPRECATED - function name can be specified with
                        --data-loader-script like so:
                        `my_custom_script.py:my_func`] When using a data-
                        loader-script, this specifies the name of the function
                        that loads data. Defaults to `load_data`.
