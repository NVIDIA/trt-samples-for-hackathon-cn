usage: polygraphy surgeon extract [-h] [-v] [-q]
                                  [--verbosity VERBOSITY [VERBOSITY ...]]
                                  [--silent]
                                  [--log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]]
                                  [--log-file LOG_FILE]
                                  [--model-input-shapes INPUT_SHAPES [INPUT_SHAPES ...]]
                                  [--seed SEED]
                                  [--val-range VAL_RANGE [VAL_RANGE ...]]
                                  [--int-min INT_MIN] [--int-max INT_MAX]
                                  [--float-min FLOAT_MIN]
                                  [--float-max FLOAT_MAX] [--iterations NUM]
                                  [--data-loader-backend-module {numpy,torch}]
                                  [--load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                                  | --data-loader-script DATA_LOADER_SCRIPT]
                                  [--data-loader-func-name DATA_LOADER_FUNC_NAME]
                                  [--shape-inference | --force-fallback-shape-inference]
                                  [--no-onnxruntime-shape-inference]
                                  [--external-data-dir EXTERNAL_DATA_DIR]
                                  [--ignore-external-data] [--fp-to-fp16] -o
                                  SAVE_ONNX
                                  [--save-external-data [EXTERNAL_DATA_PATH]]
                                  [--external-data-size-threshold EXTERNAL_DATA_SIZE_THRESHOLD]
                                  [--no-save-all-tensors-to-one-file]
                                  [--inputs INPUT_META [INPUT_META ...]]
                                  [--outputs OUTPUT_META [OUTPUT_META ...]]
                                  model_file

Extract a subgraph from an ONNX model based on the specified inputs and outputs.

options:
  -h, --help            show this help message and exit
  --inputs INPUT_META [INPUT_META ...]
                        Input metadata for subgraph (names, shapes, and data
                        types). Use 'auto' to make `extract` determine these
                        automatically. Format: --inputs
                        <name>:<shape>:<dtype>. For example: --inputs
                        input0:[1,3,224,224]:float32 input1:auto:auto. If
                        omitted, uses the current model inputs.
  --outputs OUTPUT_META [OUTPUT_META ...]
                        Output metadata for subgraph (names and data types).
                        Use 'auto' to make `extract` determine these
                        automatically. Format: --outputs <name>:<dtype>. For
                        example: --outputs output0:float32 output1:auto. If
                        omitted, uses the current model outputs.

Logging:
  Options related to logging and debug output

  -v, --verbose         Increase logging verbosity. Specify multiple times for
                        higher verbosity
  -q, --quiet           Decrease logging verbosity. Specify multiple times for
                        lower verbosity
  --verbosity VERBOSITY [VERBOSITY ...]
                        The logging verbosity to use. Takes precedence over
                        the `-v` and `-q` options, and unlike them, allows you
                        to control per-path verbosity. Verbosity values should
                        come from Polygraphy's logging verbosities defined in
                        the `Logger` class and are case-insensitive. For
                        example: `--verbosity INFO` or `--verbosity verbose`.
                        To specify per-path verbosity, use the format:
                        `<path>:<verbosity>`. For example: `--verbosity
                        backend/trt:INFO backend/trt/loader.py:VERBOSE`. Paths
                        should be relative to the `polygraphy/` directory. For
                        example, `polygraphy/backend` should be specified with
                        just `backend`. The most closely matching path is used
                        to determine verbosity. For example, with:
                        `--verbosity warning backend:info
                        backend/trt:verbose`, a file under
                        `polygraphy/comparator` would use `WARNING` verbosity,
                        one under `backend/onnx` would use `INFO`, and one
                        under `backend/trt` would use `VERBOSE`.
  --silent              Disable all output
  --log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]
                        Format for log messages: {{'timestamp': Include
                        timestamp, 'line-info': Include file and line number,
                        'no-colors': Disable colors}}
  --log-file LOG_FILE   Path to a file where Polygraphy logging output should
                        be written. This may not include logging output from
                        dependencies, like TensorRT or ONNX-Runtime.

Model:
  Options related to the model

  model_file            Path to the model
  --model-input-shapes INPUT_SHAPES [INPUT_SHAPES ...], --model-inputs INPUT_SHAPES [INPUT_SHAPES ...]
                        Input shapes to use when generating data to run
                        fallback shape inference. Has no effect if fallback
                        shape inference is not run. Format: --model-input-
                        shapes <name>:<shape>. For example: --model-input-
                        shapes image:[1,3,224,224] other_input:[10]

Data Loader:
  Options related to loading or generating input data for inference.

  --seed SEED           Seed to use for random inputs
  --val-range VAL_RANGE [VAL_RANGE ...]
                        Range of values to generate in the data loader. To
                        specify per-input ranges, use the format: --val-range
                        <input_name>:[min,max]. If no input name is provided,
                        the range is used for any inputs not explicitly
                        specified. For example: --val-range [0,1] inp0:[2,50]
                        inp1:[3.0,4.6]
  --int-min INT_MIN     [DEPRECATED: Use --val-range] Minimum integer value
                        for random integer inputs
  --int-max INT_MAX     [DEPRECATED: Use --val-range] Maximum integer value
                        for random integer inputs
  --float-min FLOAT_MIN
                        [DEPRECATED: Use --val-range] Minimum float value for
                        random float inputs
  --float-max FLOAT_MAX
                        [DEPRECATED: Use --val-range] Maximum float value for
                        random float inputs
  --iterations NUM, --iters NUM
                        Number of inference iterations for which the default
                        data loader should supply data
  --data-loader-backend-module {numpy,torch}
                        The module to use for generating input arrays.
                        Currently supported options: numpy, torch
  --load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...], --load-input-data LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                        Path(s) to load inputs. The file(s) should be a JSON-
                        ified List[Dict[str, numpy.ndarray]], i.e. a list
                        where each element is the feed_dict for a single
                        iteration. When this option is used, all other data
                        loader arguments are ignored.
  --data-loader-script DATA_LOADER_SCRIPT
                        Path to a Python script that defines a function that
                        loads input data. The function should take no
                        arguments and return a generator or iterable that
                        yields input data (Dict[str, np.ndarray]). When this
                        option is used, all other data loader arguments are
                        ignored. By default, Polygraphy looks for a function
                        called `load_data`. You can specify a custom function
                        name by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --data-loader-func-name DATA_LOADER_FUNC_NAME
                        [DEPRECATED - function name can be specified with
                        --data-loader-script like so:
                        `my_custom_script.py:my_func`] When using a data-
                        loader-script, this specifies the name of the function
                        that loads data. Defaults to `load_data`.

ONNX Shape Inference:
  Options related to ONNX shape inference.

  --shape-inference, --do-shape-inference
                        Enable ONNX shape inference when loading the model
  --force-fallback-shape-inference
                        Force Polygraphy to use ONNX-Runtime to determine
                        metadata for tensors in the graph. This can be useful
                        in cases where ONNX shape inference does not generate
                        correct information. Note that this will cause dynamic
                        dimensions to become static.
  --no-onnxruntime-shape-inference
                        Disable using ONNX-Runtime's shape inference
                        utilities. This will force Polygraphy to use
                        `onnx.shape_inference` instead. Note that ONNX-
                        Runtime's shape inference utilities may be more
                        performant and memory-efficient.

ONNX Model Loading:
  Options related to loading ONNX models.

  --external-data-dir EXTERNAL_DATA_DIR, --load-external-data EXTERNAL_DATA_DIR, --ext EXTERNAL_DATA_DIR
                        Path to a directory containing external data for the
                        model. Generally, this is only required if the
                        external data is not stored in the model directory.
  --ignore-external-data
                        Ignore external data and just load the model structure
                        without any weights. The model will be usable only for
                        purposes that don't require weights, such as
                        extracting subgraphs or inspecting model structure.
                        This can be useful in cases where external data is not
                        available.
  --fp-to-fp16          Convert all floating point tensors in an ONNX model to
                        16-bit precision. This is *not* needed in order to use
                        TensorRT's fp16 precision, but may be useful for other
                        backends. Requires onnxmltools.

ONNX Model Saving:
  Options related to saving ONNX models.

  -o SAVE_ONNX, --output SAVE_ONNX
                        Path to save the ONNX model
  --save-external-data [EXTERNAL_DATA_PATH], --external-data-path [EXTERNAL_DATA_PATH]
                        Whether to save weight data in external file(s). To
                        use a non-default path, supply the desired path as an
                        argument. This is always a relative path; external
                        data is always written to the same directory as the
                        model.
  --external-data-size-threshold EXTERNAL_DATA_SIZE_THRESHOLD
                        The size threshold, in bytes, above which tensor data
                        will be stored in the external file. Tensors smaller
                        that this threshold will remain in the ONNX file.
                        Optionally, use a `K`, `M`, or `G` suffix to indicate
                        KiB, MiB, or GiB respectively. For example,
                        `--external-data-size-threshold=16M` is equivalent to
                        `--external-data-size-threshold=16777216`. Has no
                        effect if `--save-external-data` is not set. Defaults
                        to 1024 bytes.
  --no-save-all-tensors-to-one-file
                        Do not save all tensors to a single file when saving
                        external data. Has no effect if `--save-external-data`
                        is not set
