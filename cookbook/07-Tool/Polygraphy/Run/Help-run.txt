usage: polygraphy run [-h] [-v] [-q] [--verbosity VERBOSITY [VERBOSITY ...]]
                      [--silent]
                      [--log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]]
                      [--log-file LOG_FILE] [--tf] [--onnxrt] [--pluginref]
                      [--trt]
                      [--model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}]
                      [--input-shapes INPUT_SHAPES [INPUT_SHAPES ...]]
                      [--tftrt] [--minimum-segment-size MINIMUM_SEGMENT_SIZE]
                      [--dynamic-op] [--ckpt CKPT]
                      [--tf-outputs TF_OUTPUTS [TF_OUTPUTS ...]]
                      [--save-pb SAVE_FROZEN_GRAPH_PATH]
                      [--save-tensorboard SAVE_TENSORBOARD_PATH]
                      [--freeze-graph]
                      [--gpu-memory-fraction GPU_MEMORY_FRACTION]
                      [--allow-growth] [--xla] [--save-timeline SAVE_TIMELINE]
                      [--opset OPSET] [--save-onnx SAVE_ONNX]
                      [--save-external-data [EXTERNAL_DATA_PATH]]
                      [--external-data-size-threshold EXTERNAL_DATA_SIZE_THRESHOLD]
                      [--no-save-all-tensors-to-one-file] [--shape-inference]
                      [--no-onnxruntime-shape-inference]
                      [--external-data-dir EXTERNAL_DATA_DIR]
                      [--ignore-external-data]
                      [--onnx-outputs ONNX_OUTPUTS [ONNX_OUTPUTS ...]]
                      [--onnx-exclude-outputs ONNX_EXCLUDE_OUTPUTS [ONNX_EXCLUDE_OUTPUTS ...]]
                      [--fp-to-fp16] [--providers PROVIDERS [PROVIDERS ...]]
                      [--trt-min-shapes TRT_MIN_SHAPES [TRT_MIN_SHAPES ...]]
                      [--trt-opt-shapes TRT_OPT_SHAPES [TRT_OPT_SHAPES ...]]
                      [--trt-max-shapes TRT_MAX_SHAPES [TRT_MAX_SHAPES ...]]
                      [--tf32] [--fp16] [--bf16] [--fp8] [--int8]
                      [--precision-constraints {prefer,obey,none}]
                      [--sparse-weights] [--version-compatible]
                      [--exclude-lean-runtime]
                      [--calibration-cache CALIBRATION_CACHE]
                      [--calib-base-cls CALIBRATION_BASE_CLASS]
                      [--quantile QUANTILE]
                      [--regression-cutoff REGRESSION_CUTOFF]
                      [--load-timing-cache LOAD_TIMING_CACHE]
                      [--error-on-timing-cache-miss]
                      [--disable-compilation-cache]
                      [--save-tactics SAVE_TACTICS | --load-tactics LOAD_TACTICS]
                      [--tactic-sources [TACTIC_SOURCES ...]]
                      [--trt-config-script TRT_CONFIG_SCRIPT]
                      [--trt-config-func-name TRT_CONFIG_FUNC_NAME]
                      [--trt-config-postprocess-script TRT_CONFIG_POSTPROCESS_SCRIPT]
                      [--trt-safety-restricted] [--refittable] [--strip-plan]
                      [--use-dla] [--allow-gpu-fallback]
                      [--pool-limit MEMORY_POOL_LIMIT [MEMORY_POOL_LIMIT ...]]
                      [--preview-features [PREVIEW_FEATURES ...]]
                      [--builder-optimization-level BUILDER_OPTIMIZATION_LEVEL]
                      [--hardware-compatibility-level HARDWARE_COMPATIBILITY_LEVEL]
                      [--max-aux-streams MAX_AUX_STREAMS]
                      [--quantization-flags [QUANTIZATION_FLAGS ...]]
                      [--profiling-verbosity PROFILING_VERBOSITY]
                      [--weight-streaming]
                      [--runtime-platform RUNTIME_PLATFORM]
                      [--plugins PLUGINS [PLUGINS ...]]
                      [--onnx-flags ONNX_FLAGS [ONNX_FLAGS ...]]
                      [--plugin-instancenorm]
                      [--trt-outputs TRT_OUTPUTS [TRT_OUTPUTS ...]]
                      [--trt-exclude-outputs TRT_EXCLUDE_OUTPUTS [TRT_EXCLUDE_OUTPUTS ...]]
                      [--layer-precisions LAYER_PRECISIONS [LAYER_PRECISIONS ...]]
                      [--tensor-dtypes TENSOR_DTYPES [TENSOR_DTYPES ...]]
                      [--trt-network-func-name TRT_NETWORK_FUNC_NAME]
                      [--trt-network-postprocess-script TRT_NETWORK_POSTPROCESS_SCRIPT [TRT_NETWORK_POSTPROCESS_SCRIPT ...]]
                      [--strongly-typed]
                      [--mark-debug MARK_DEBUG [MARK_DEBUG ...]]
                      [--save-engine SAVE_ENGINE]
                      [--save-timing-cache SAVE_TIMING_CACHE]
                      [--load-runtime LOAD_RUNTIME]
                      [--optimization-profile OPTIMIZATION_PROFILE]
                      [--allocation-strategy {static,profile,runtime}]
                      [--weight-streaming-budget WEIGHT_STREAMING_BUDGET]
                      [--seed SEED] [--val-range VAL_RANGE [VAL_RANGE ...]]
                      [--int-min INT_MIN] [--int-max INT_MAX]
                      [--float-min FLOAT_MIN] [--float-max FLOAT_MAX]
                      [--iterations NUM]
                      [--data-loader-backend-module {numpy,torch}]
                      [--load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                      | --data-loader-script DATA_LOADER_SCRIPT]
                      [--data-loader-func-name DATA_LOADER_FUNC_NAME]
                      [--warm-up NUM] [--use-subprocess]
                      [--save-inputs SAVE_INPUTS_PATH]
                      [--save-outputs SAVE_OUTPUTS_PATH]
                      [--postprocess POSTPROCESS [POSTPROCESS ...]]
                      [--validate] [--fail-fast] [--compare {simple,indices}]
                      [--compare-func-script COMPARE_FUNC_SCRIPT]
                      [--load-outputs LOAD_OUTPUTS_PATHS [LOAD_OUTPUTS_PATHS ...]]
                      [--no-shape-check] [--rtol RTOL [RTOL ...]]
                      [--atol ATOL [ATOL ...]]
                      [--check-error-stat CHECK_ERROR_STAT [CHECK_ERROR_STAT ...]]
                      [--infinities-compare-equal]
                      [--save-heatmaps SAVE_HEATMAPS] [--show-heatmaps]
                      [--save-error-metrics-plot SAVE_ERROR_METRICS_PLOT]
                      [--show-error-metrics-plot]
                      [--error-quantile ERROR_QUANTILE [ERROR_QUANTILE ...]]
                      [--index-tolerance INDEX_TOLERANCE [INDEX_TOLERANCE ...]]
                      [--gen GEN_SCRIPT]
                      [model_file]

Run inference and compare results across backends.

The typical usage of `run` is:

    polygraphy run [model_file] [runners...] [runner_options...]

`run` will then run inference on the specified model with all the specified runners
and compare inference outputs between them.

TIP: You can use `--gen-script` to generate a Python script that does exactly what the `run`
command would otherwise do.

options:
  -h, --help            show this help message and exit
  --gen GEN_SCRIPT, --gen-script GEN_SCRIPT
                        Path to save a generated Python script, that will do
                        exactly what `run` would. When this option is enabled,
                        `run` will save the script and exit. Use a value of
                        `-` to print the script to the standard output instead
                        of saving it to a file

Logging:
  Options related to logging and debug output

  -v, --verbose         Increase logging verbosity. Specify multiple times for
                        higher verbosity
  -q, --quiet           Decrease logging verbosity. Specify multiple times for
                        lower verbosity
  --verbosity VERBOSITY [VERBOSITY ...]
                        The logging verbosity to use. Takes precedence over
                        the `-v` and `-q` options, and unlike them, allows you
                        to control per-path verbosity. Verbosity values should
                        come from Polygraphy's logging verbosities defined in
                        the `Logger` class and are case-insensitive. For
                        example: `--verbosity INFO` or `--verbosity verbose`.
                        To specify per-path verbosity, use the format:
                        `<path>:<verbosity>`. For example: `--verbosity
                        backend/trt:INFO backend/trt/loader.py:VERBOSE`. Paths
                        should be relative to the `polygraphy/` directory. For
                        example, `polygraphy/backend` should be specified with
                        just `backend`. The most closely matching path is used
                        to determine verbosity. For example, with:
                        `--verbosity warning backend:info
                        backend/trt:verbose`, a file under
                        `polygraphy/comparator` would use `WARNING` verbosity,
                        one under `backend/onnx` would use `INFO`, and one
                        under `backend/trt` would use `VERBOSE`.
  --silent              Disable all output
  --log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]
                        Format for log messages: {{'timestamp': Include
                        timestamp, 'line-info': Include file and line number,
                        'no-colors': Disable colors}}
  --log-file LOG_FILE   Path to a file where Polygraphy logging output should
                        be written. This may not include logging output from
                        dependencies, like TensorRT or ONNX-Runtime.

Runners:
  Options related to selecting runners to use for inference.

  --tf                  Run inference using TensorFlow.
  --onnxrt              Run inference using ONNX-Runtime.
  --pluginref           Run inference using Plugin CPU Reference.
  --trt                 Run inference using TensorRT.

Model:
  Options related to the model

  model_file            Path to the model
  --model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}
                        The type of the input model: {{'frozen': TensorFlow
                        frozen graph; 'keras': Keras model; 'ckpt': TensorFlow
                        checkpoint directory; 'onnx': ONNX model; 'engine':
                        TensorRT engine; 'trt-network-script': A Python script
                        that defines a `load_network` function that takes no
                        arguments and returns a TensorRT Builder, Network, and
                        optionally Parser. If the function name is not
                        `load_network`, it can be specified after the model
                        file, separated by a colon. For example:
                        `my_custom_script.py:my_func`; 'uff': UFF file
                        [deprecated]; 'caffe': Caffe prototxt [deprecated]}}
  --input-shapes INPUT_SHAPES [INPUT_SHAPES ...], --inputs INPUT_SHAPES [INPUT_SHAPES ...]
                        Model input(s) and their shape(s). Used to determine
                        shapes to use while generating input data for
                        inference. Format: --input-shapes <name>:<shape>. For
                        example: --input-shapes image:[1,3,224,224]
                        other_input:[10]

[UNTESTED] TensorFlow-TensorRT Integration:
  Options related to TensorFlow-TensorRT.

  --tftrt, --use-tftrt  Enable TF-TRT integration
  --minimum-segment-size MINIMUM_SEGMENT_SIZE
                        Minimum length of a segment to convert to TensorRT
  --dynamic-op          Enable dynamic mode (defers engine build until
                        runtime)

TensorFlow Model Loading:
  Options related to loading TensorFlow models.

  --ckpt CKPT           [EXPERIMENTAL] Name of the checkpoint to load.
                        Required if the `checkpoint` file is missing. Should
                        not include file extension (e.g. to load `model.meta`
                        use `--ckpt=model`)
  --tf-outputs TF_OUTPUTS [TF_OUTPUTS ...]
                        Name(s) of TensorFlow output(s). Using '--tf-outputs
                        mark all' indicates that all tensors should be used as
                        outputs
  --save-pb SAVE_FROZEN_GRAPH_PATH
                        Path to save the TensorFlow frozen graphdef
  --save-tensorboard SAVE_TENSORBOARD_PATH
                        [EXPERIMENTAL] Path to save a TensorBoard
                        visualization
  --freeze-graph        [EXPERIMENTAL] Attempt to freeze the graph

TensorFlow Session Configuration:
  Options related to creating the TensorFlow SessionConfig.

  --gpu-memory-fraction GPU_MEMORY_FRACTION
                        Maximum percentage of GPU memory TensorFlow can
                        allocate per process
  --allow-growth        Allow GPU memory allocated by TensorFlow to grow
  --xla                 [EXPERIMENTAL] Attempt to run graph with xla

TensorFlow Inference:
  Options related to running inference with TensorFlow.

  --save-timeline SAVE_TIMELINE
                        [EXPERIMENTAL] Directory to save timeline JSON files
                        for profiling inference (view at chrome://tracing)

TensorFlow-ONNX Model Conversion:
  Options related to converting TensorFlow models to ONNX.

  --opset OPSET         Opset to use when converting to ONNX

ONNX Model Saving:
  Options related to saving ONNX models.

  --save-onnx SAVE_ONNX
                        Path to save the ONNX model
  --save-external-data [EXTERNAL_DATA_PATH], --external-data-path [EXTERNAL_DATA_PATH]
                        Whether to save weight data in external file(s). To
                        use a non-default path, supply the desired path as an
                        argument. This is always a relative path; external
                        data is always written to the same directory as the
                        model.
  --external-data-size-threshold EXTERNAL_DATA_SIZE_THRESHOLD
                        The size threshold, in bytes, above which tensor data
                        will be stored in the external file. Tensors smaller
                        that this threshold will remain in the ONNX file.
                        Optionally, use a `K`, `M`, or `G` suffix to indicate
                        KiB, MiB, or GiB respectively. For example,
                        `--external-data-size-threshold=16M` is equivalent to
                        `--external-data-size-threshold=16777216`. Has no
                        effect if `--save-external-data` is not set. Defaults
                        to 1024 bytes.
  --no-save-all-tensors-to-one-file
                        Do not save all tensors to a single file when saving
                        external data. Has no effect if `--save-external-data`
                        is not set

ONNX Shape Inference:
  Options related to ONNX shape inference.

  --shape-inference, --do-shape-inference
                        Enable ONNX shape inference when loading the model
  --no-onnxruntime-shape-inference
                        Disable using ONNX-Runtime's shape inference
                        utilities. This will force Polygraphy to use
                        `onnx.shape_inference` instead. Note that ONNX-
                        Runtime's shape inference utilities may be more
                        performant and memory-efficient.

ONNX Model Loading:
  Options related to loading ONNX models.

  --external-data-dir EXTERNAL_DATA_DIR, --load-external-data EXTERNAL_DATA_DIR, --ext EXTERNAL_DATA_DIR
                        Path to a directory containing external data for the
                        model. Generally, this is only required if the
                        external data is not stored in the model directory.
  --ignore-external-data
                        Ignore external data and just load the model structure
                        without any weights. The model will be usable only for
                        purposes that don't require weights, such as
                        extracting subgraphs or inspecting model structure.
                        This can be useful in cases where external data is not
                        available.
  --onnx-outputs ONNX_OUTPUTS [ONNX_OUTPUTS ...]
                        Name(s) of ONNX tensor(s) to mark as output(s). Using
                        the special value 'mark all' indicates that all
                        tensors should be used as outputs
  --onnx-exclude-outputs ONNX_EXCLUDE_OUTPUTS [ONNX_EXCLUDE_OUTPUTS ...]
                        [EXPERIMENTAL] Name(s) of ONNX output(s) to unmark as
                        outputs.
  --fp-to-fp16          Convert all floating point tensors in an ONNX model to
                        16-bit precision. This is *not* needed in order to use
                        TensorRT's fp16 precision, but may be useful for other
                        backends. Requires onnxmltools.

ONNX-Runtime Session Creation:
  Options related to creating an ONNX-Runtime Inference Session

  --providers PROVIDERS [PROVIDERS ...], --execution-providers PROVIDERS [PROVIDERS ...]
                        A list of execution providers to use in order of
                        priority. Each provider may be either an exact match
                        or a case-insensitive partial match for the execution
                        providers available in ONNX-Runtime. For example, a
                        value of 'cpu' would match the 'CPUExecutionProvider'

TensorRT Builder Configuration:
  Options related to creating the TensorRT BuilderConfig.

  --trt-min-shapes TRT_MIN_SHAPES [TRT_MIN_SHAPES ...]
                        The minimum shapes the optimization profile(s) will
                        support. Specify this option once for each profile. If
                        not provided, inference-time input shapes are used.
                        Format: --trt-min-shapes <input0>:[D0,D1,..,DN] ..
                        <inputN>:[D0,D1,..,DN]
  --trt-opt-shapes TRT_OPT_SHAPES [TRT_OPT_SHAPES ...]
                        The shapes for which the optimization profile(s) will
                        be most performant. Specify this option once for each
                        profile. If not provided, inference-time input shapes
                        are used. Format: --trt-opt-shapes
                        <input0>:[D0,D1,..,DN] .. <inputN>:[D0,D1,..,DN]
  --trt-max-shapes TRT_MAX_SHAPES [TRT_MAX_SHAPES ...]
                        The maximum shapes the optimization profile(s) will
                        support. Specify this option once for each profile. If
                        not provided, inference-time input shapes are used.
                        Format: --trt-max-shapes <input0>:[D0,D1,..,DN] ..
                        <inputN>:[D0,D1,..,DN]
  --tf32                Enable tf32 precision in TensorRT
  --fp16                Enable fp16 precision in TensorRT
  --bf16                Enable bf16 precision in TensorRT
  --fp8                 Enable fp8 precision in TensorRT
  --int8                Enable int8 precision in TensorRT. If calibration is
                        required but no calibration cache is provided, this
                        option will cause TensorRT to run int8 calibration
                        using the Polygraphy data loader to provide
                        calibration data. If calibration is run and the model
                        has dynamic shapes, the last optimization profile will
                        be used as the calibration profile.
  --precision-constraints {prefer,obey,none}
                        If set to `prefer`, TensorRT will restrict available
                        tactics to layer precisions specified in the network
                        unless no implementation exists with the preferred
                        layer constraints, in which case it will issue a
                        warning and use the fastest available implementation.
                        If set to `obey`, TensorRT will instead fail to build
                        the network if no implementation exists with the
                        preferred layer constraints. Defaults to `none`
  --sparse-weights      Enable optimizations for sparse weights in TensorRT
  --version-compatible  Builds an engine designed to be forward TensorRT
                        version compatible.
  --exclude-lean-runtime
                        Exclude the lean runtime from the plan when version
                        compatibility is enabled.
  --calibration-cache CALIBRATION_CACHE
                        Path to load/save a calibration cache. Used to store
                        calibration scales to speed up the process of int8
                        calibration. If the provided path does not yet exist,
                        int8 calibration scales will be calculated and written
                        to it during engine building. If the provided path
                        does exist, it will be read and int8 calibration will
                        be skipped during engine building.
  --calib-base-cls CALIBRATION_BASE_CLASS, --calibration-base-class CALIBRATION_BASE_CLASS
                        The name of the calibration base class to use. For
                        example, 'IInt8MinMaxCalibrator'.
  --quantile QUANTILE   The quantile to use for IInt8LegacyCalibrator. Has no
                        effect for other calibrator types.
  --regression-cutoff REGRESSION_CUTOFF
                        The regression cutoff to use for
                        IInt8LegacyCalibrator. Has no effect for other
                        calibrator types.
  --load-timing-cache LOAD_TIMING_CACHE
                        Path to load tactic timing cache. Used to cache tactic
                        timing information to speed up the engine building
                        process. If the file specified by --load-timing-cache
                        does not exist, Polygraphy will emit a warning and
                        fall back to using an empty timing cache.
  --error-on-timing-cache-miss
                        Emit error when a tactic being timed is not present in
                        the timing cache.
  --disable-compilation-cache
                        Disable caching JIT-compiled code
  --save-tactics SAVE_TACTICS, --save-tactic-replay SAVE_TACTICS
                        Path to save a Polygraphy tactic replay file. Details
                        about tactics selected by TensorRT will be recorded
                        and stored at this location as a JSON file.
  --load-tactics LOAD_TACTICS, --load-tactic-replay LOAD_TACTICS
                        Path to load a Polygraphy tactic replay file, such as
                        one created by --save-tactics. The tactics specified
                        in the file will be used to override TensorRT's
                        default selections.
  --tactic-sources [TACTIC_SOURCES ...]
                        Tactic sources to enable. This controls which
                        libraries (e.g. cudnn, cublas, etc.) TensorRT is
                        allowed to load tactics from. Values come from the
                        names of the values in the trt.TacticSource enum and
                        are case-insensitive. If no arguments are provided,
                        e.g. '--tactic-sources', then all tactic sources are
                        disabled.Defaults to TensorRT's default tactic
                        sources.
  --trt-config-script TRT_CONFIG_SCRIPT
                        Path to a Python script that defines a function that
                        creates a TensorRT IBuilderConfig. The function should
                        take a builder and network as parameters and return a
                        TensorRT builder configuration. When this option is
                        specified, all other config arguments are ignored. By
                        default, Polygraphy looks for a function called
                        `load_config`. You can specify a custom function name
                        by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --trt-config-func-name TRT_CONFIG_FUNC_NAME
                        [DEPRECATED - function name can be specified with
                        --trt-config-script like so:
                        `my_custom_script.py:my_func`]When using a trt-config-
                        script, this specifies the name of the function that
                        creates the config. Defaults to `load_config`.
  --trt-config-postprocess-script TRT_CONFIG_POSTPROCESS_SCRIPT, --trt-cpps TRT_CONFIG_POSTPROCESS_SCRIPT
                        [EXPERIMENTAL] Path to a Python script that defines a
                        function that modifies a TensorRT IBuilderConfig. This
                        function will be called after Polygraphy has finished
                        created the builder configuration and should take a
                        builder, network, and config as parameters and modify
                        the config in place. Unlike `--trt-config-script`, all
                        other config arguments will be reflected in the config
                        passed to the function.By default, Polygraphy looks
                        for a function called `postprocess_config`. You can
                        specify a custom function name by separating it with a
                        colon. For example: `my_custom_script.py:my_func`
  --trt-safety-restricted
                        Enable safety scope checking in TensorRT
  --refittable          Enable the engine to be refitted with new weights
                        after it is built.
  --strip-plan          Builds the engine with the refittable weights
                        stripped.
  --use-dla             [EXPERIMENTAL] Use DLA as the default device type
  --allow-gpu-fallback  [EXPERIMENTAL] Allow layers unsupported on the DLA to
                        fall back to GPU. Has no effect if --use-dla is not
                        set.
  --pool-limit MEMORY_POOL_LIMIT [MEMORY_POOL_LIMIT ...], --memory-pool-limit MEMORY_POOL_LIMIT [MEMORY_POOL_LIMIT ...]
                        Memory pool limits. Memory pool names come from the
                        names of values in the `trt.MemoryPoolType` enum and
                        are case-insensitiveFormat: `--pool-limit
                        <pool_name>:<pool_limit> ...`. For example, `--pool-
                        limit dla_local_dram:1e9 workspace:16777216`.
                        Optionally, use a `K`, `M`, or `G` suffix to indicate
                        KiB, MiB, or GiB respectively. For example, `--pool-
                        limit workspace:16M` is equivalent to `--pool-limit
                        workspace:16777216`.
  --preview-features [PREVIEW_FEATURES ...]
                        Preview features to enable. Values come from the names
                        of the values in the trt.PreviewFeature enum, and are
                        case-insensitive.If no arguments are provided, e.g. '
                        --preview-features', then all preview features are
                        disabled. Defaults to TensorRT's default preview
                        features.
  --builder-optimization-level BUILDER_OPTIMIZATION_LEVEL
                        The builder optimization level. Setting a higher
                        optimization level allows the optimizer to spend more
                        time searching for optimization opportunities. The
                        resulting engine may have better performance compared
                        to an engine built with a lower optimization level.
                        Refer to the TensorRT API documentation for details.
  --hardware-compatibility-level HARDWARE_COMPATIBILITY_LEVEL
                        The hardware compatibility level to use for the
                        engine. This allows engines built on one GPU
                        architecture to work on GPUs of other architectures.
                        Values come from the names of values in the
                        `trt.HardwareCompatibilityLevel` enum and are case-
                        insensitive. For example, `--hardware-compatibility-
                        level ampere_plus`
  --max-aux-streams MAX_AUX_STREAMS
                        The maximum number of auxiliary streams that TensorRT
                        is allowed to use. If the network contains operators
                        that can run in parallel, TRT can execute them using
                        auxiliary streams in addition to the one provided to
                        the IExecutionContext.execute_async_v3() call. The
                        default maximum number of auxiliary streams is
                        determined by the heuristics in TensorRT on whether
                        enabling multi-stream would improve the performance.
                        Refer to the TensorRT API documentation for details.
  --quantization-flags [QUANTIZATION_FLAGS ...]
                        Int8 quantization flags to enable. Values come from
                        the names of values in the trt.QuantizationFlag enum,
                        and are case-insensitive. If no arguments are
                        provided, e.g. '--quantization-flags', then all
                        quantization flags are disabled. Defaults to
                        TensorRT's default quantization flags.
  --profiling-verbosity PROFILING_VERBOSITY
                        The verbosity of NVTX annotations in the generated
                        engine.Values come from the names of values in the
                        `trt.ProfilingVerbosity` enum and are case-
                        insensitive. For example, `--profiling-verbosity
                        detailed`. Defaults to 'verbose'.
  --weight-streaming    Build a weight streamable engine. Must be set with
                        --strongly-typed. The weight streaming amount can be
                        set with --weight-streaming-budget.
  --runtime-platform RUNTIME_PLATFORM
                        The target runtime platform (operating system and CPU
                        architecture) for the execution of the TensorRT
                        engine. TensorRT provides support for cross-platform
                        engine compatibility when the target runtime platform
                        is different from the build platform. Values come from
                        the names of values in the `trt.RuntimePlatform` enum
                        and are case-insensitive. For example, `--runtime-
                        platform same_as_build`, `--runtime-platform
                        windows_amd64`

TensorRT Plugin Loading:
  Options related to loading TensorRT plugins.

  --plugins PLUGINS [PLUGINS ...]
                        Path(s) of plugin libraries to load

ONNX-TRT Parser Flags:
  Options related to setting flags for TensorRT's ONNX parser

  --onnx-flags ONNX_FLAGS [ONNX_FLAGS ...]
                        Flag(s) for adjusting the default parsing behavior of
                        the ONNX parser.Flag values come from the
                        `trt.OnnxParserFlag` enum and are case-insensitve.For
                        example: --onnx-flags native_instancenorm
  --plugin-instancenorm
                        Switch to clear the
                        `trt.OnnxParserFlag.NATIVE_INSTANCENORM` flag andforce
                        the usage of the plugin implementation of ONNX
                        InstanceNorm.Note that
                        `trt.OnnxParserFlag.NATIVE_INSTANCENORM` is ON by
                        default since TensorRT 10.0.

TensorRT Network Loading:
  Options related to loading TensorRT networks.

  --trt-outputs TRT_OUTPUTS [TRT_OUTPUTS ...]
                        Name(s) of TensorRT output(s). Using '--trt-outputs
                        mark all' indicates that all tensors should be used as
                        outputs
  --trt-exclude-outputs TRT_EXCLUDE_OUTPUTS [TRT_EXCLUDE_OUTPUTS ...]
                        [EXPERIMENTAL] Name(s) of TensorRT output(s) to unmark
                        as outputs.
  --layer-precisions LAYER_PRECISIONS [LAYER_PRECISIONS ...]
                        Compute precision to use for each layer. This should
                        be specified on a per-layer basis, using the format:
                        --layer-precisions <layer_name>:<layer_precision>.
                        Precision values come from the TensorRT data type
                        aliases, like float32, float16, int8, bool, etc. For
                        example: --layer-precisions example_layer:float16
                        other_layer:int8. When this option is provided, you
                        should also set --precision-constraints to either
                        'prefer' or 'obey'.
  --tensor-dtypes TENSOR_DTYPES [TENSOR_DTYPES ...], --tensor-datatypes TENSOR_DTYPES [TENSOR_DTYPES ...]
                        Data type to use for each network I/O tensor. This
                        should be specified on a per-tensor basis, using the
                        format: --tensor-datatypes
                        <tensor_name>:<tensor_datatype>. Data type values come
                        from the TensorRT data type aliases, like float32,
                        float16, int8, bool, etc. For example: --tensor-
                        datatypes example_tensor:float16 other_tensor:int8.
  --trt-network-func-name TRT_NETWORK_FUNC_NAME
                        [DEPRECATED - function name can be specified alongside
                        the script like so: `my_custom_script.py:my_func`]
                        When using a trt-network-script instead of other model
                        types, this specifies the name of the function that
                        loads the network. Defaults to `load_network`.
  --trt-network-postprocess-script TRT_NETWORK_POSTPROCESS_SCRIPT [TRT_NETWORK_POSTPROCESS_SCRIPT ...], --trt-npps TRT_NETWORK_POSTPROCESS_SCRIPT [TRT_NETWORK_POSTPROCESS_SCRIPT ...]
                        [EXPERIMENTAL] Specify a post-processing script to run
                        on the parsed TensorRT network. The script file may
                        optionally be suffixed with the name of the callable
                        to be invoked. For example: `--trt-npps
                        process.py:do_something`. If no callable is specified,
                        then by default Polygraphy uses the callable name
                        `postprocess`. The callable is expected to take a
                        named argument `network` of type
                        `trt.INetworkDefinition`. Multiple scripts may be
                        specified, in which case they are executed in the
                        order given.
  --strongly-typed      Mark the network as being strongly typed.
  --mark-debug MARK_DEBUG [MARK_DEBUG ...]
                        Specify list of names of tensors to be marked as debug
                        tensors.For example, `--mark-debug tensor1 tensor2
                        tensor3`.

TensorRT Engine Saving:
  Options related to saving TensorRT engines.

  --save-engine SAVE_ENGINE
                        Path to save the TensorRT Engine

TensorRT Engine:
  Options related to loading or building TensorRT engines.

  --save-timing-cache SAVE_TIMING_CACHE
                        Path to save tactic timing cache if building an
                        engine. Existing caches will be appended to with any
                        new timing information gathered.

TensorRT Engine:
  Options related to loading TensorRT engines.

  --load-runtime LOAD_RUNTIME
                        Path from which to load a runtime that can be used to
                        load a version compatible engine that excludes the
                        lean runtime.

TensorRT Inference:
  Options related to running inference with TensorRT.

  --optimization-profile OPTIMIZATION_PROFILE
                        The index of optimization profile to use for inference
  --allocation-strategy {static,profile,runtime}
                        The way activation memory is allocated. static: Pre-
                        allocate based on the max possible size across all
                        profiles. profile: Allocate what's needed for the
                        profile to use.runtime: Allocate what's needed for the
                        current input shapes.
  --weight-streaming-budget WEIGHT_STREAMING_BUDGET
                        The amount of GPU memory in bytes that TensorRT can
                        use for weights at runtime. The engine must be built
                        with weight streaming enabled. It can take on the
                        following values: None or -2: Disables weight
                        streaming at runtime. -1: TensorRT will decide the
                        streaming budget automatically. 0 to 100%: The
                        percentage of weights that TRT keeps on the GPU. 0%
                        will stream the maximum number of weights.>=0B: The
                        exact amount of streamable weights that reside on the
                        GPU (unit suffixes are supported).

Data Loader:
  Options related to loading or generating input data for inference.

  --seed SEED           Seed to use for random inputs
  --val-range VAL_RANGE [VAL_RANGE ...]
                        Range of values to generate in the data loader. To
                        specify per-input ranges, use the format: --val-range
                        <input_name>:[min,max]. If no input name is provided,
                        the range is used for any inputs not explicitly
                        specified. For example: --val-range [0,1] inp0:[2,50]
                        inp1:[3.0,4.6]
  --int-min INT_MIN     [DEPRECATED: Use --val-range] Minimum integer value
                        for random integer inputs
  --int-max INT_MAX     [DEPRECATED: Use --val-range] Maximum integer value
                        for random integer inputs
  --float-min FLOAT_MIN
                        [DEPRECATED: Use --val-range] Minimum float value for
                        random float inputs
  --float-max FLOAT_MAX
                        [DEPRECATED: Use --val-range] Maximum float value for
                        random float inputs
  --iterations NUM, --iters NUM
                        Number of inference iterations for which the default
                        data loader should supply data
  --data-loader-backend-module {numpy,torch}
                        The module to use for generating input arrays.
                        Currently supported options: numpy, torch
  --load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...], --load-input-data LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                        Path(s) to load inputs. The file(s) should be a JSON-
                        ified List[Dict[str, numpy.ndarray]], i.e. a list
                        where each element is the feed_dict for a single
                        iteration. When this option is used, all other data
                        loader arguments are ignored.
  --data-loader-script DATA_LOADER_SCRIPT
                        Path to a Python script that defines a function that
                        loads input data. The function should take no
                        arguments and return a generator or iterable that
                        yields input data (Dict[str, np.ndarray]). When this
                        option is used, all other data loader arguments are
                        ignored. By default, Polygraphy looks for a function
                        called `load_data`. You can specify a custom function
                        name by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --data-loader-func-name DATA_LOADER_FUNC_NAME
                        [DEPRECATED - function name can be specified with
                        --data-loader-script like so:
                        `my_custom_script.py:my_func`] When using a data-
                        loader-script, this specifies the name of the function
                        that loads data. Defaults to `load_data`.

Comparator Inference:
  Options related to running inference via ``Comparator.run()``.

  --warm-up NUM         Number of warm-up runs before timing inference
  --use-subprocess      Run runners in isolated subprocesses. Cannot be used
                        with a debugger
  --save-inputs SAVE_INPUTS_PATH, --save-input-data SAVE_INPUTS_PATH
                        Path to save inference inputs. The inputs
                        (List[Dict[str, numpy.ndarray]]) will be encoded as
                        JSON and saved
  --save-outputs SAVE_OUTPUTS_PATH, --save-results SAVE_OUTPUTS_PATH
                        Path to save results from runners. The results
                        (RunResults) will be encoded as JSON and saved

Comparator Postprocessing:
  Options related to applying postprocessing to outputs.

  --postprocess POSTPROCESS [POSTPROCESS ...], --postprocess-func POSTPROCESS [POSTPROCESS ...]
                        Apply post-processing on the specified outputs prior
                        to comparison. Format: --postprocess
                        [<out_name>:]<func>. If no output name is provided,
                        the function is applied to all outputs. For example:
                        `--postprocess out0:top-5 out1:top-3` or
                        `--postprocess top-5`. Available post-processing
                        functions are: {{top-<K>[,axis=<axis>]: Takes the
                        indices of the K highest values along the specified
                        axis (defaulting to the last axis), where K is an
                        integer. For example: `--postprocess top-5` or
                        `--postprocess top-5,axis=1`}}

Comparator Comparisons:
  Options related to inference output comparisons.

  --validate            Check outputs for NaNs and Infs
  --fail-fast           Fail fast (stop comparing after the first failure)
  --compare {simple,indices}, --compare-func {simple,indices}
                        Name of the function to use to perform comparison. See
                        the API documentation for `CompareFunc` for details.
                        Defaults to 'simple'.
  --compare-func-script COMPARE_FUNC_SCRIPT
                        [EXPERIMENTAL] Path to a Python script that defines a
                        function that can compare two iteration results. This
                        function must have a signature of: `(IterationResult,
                        IterationResult) -> OrderedDict[str, bool]`. For
                        details, see the API documentation for
                        `Comparator.compare_accuracy()`. If provided, this
                        will override all other comparison function options.
                        By default, Polygraphy looks for a function called
                        `compare_outputs`. You can specify a custom function
                        name by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --load-outputs LOAD_OUTPUTS_PATHS [LOAD_OUTPUTS_PATHS ...], --load-results LOAD_OUTPUTS_PATHS [LOAD_OUTPUTS_PATHS ...]
                        Path(s) to load results from runners prior to
                        comparing. Each file should be a JSON-ified RunResults

Comparison Function: `simple`:
  Options related to the `CompareFunc.simple` comparison function.

  --no-shape-check      Disable checking that output shapes match exactly
  --rtol RTOL [RTOL ...], --rel-tol RTOL [RTOL ...]
                        Relative tolerance for output comparison. This is
                        expressed as a percentage of the second set of output
                        values. For example, a value of 0.01 would check that
                        the first set of outputs is within 1% of the second.
                        To specify per-output tolerances, use the format:
                        --rtol [<out_name>:]<rtol>. If no output name is
                        provided, the tolerance is used for any outputs not
                        explicitly specified. For example: --rtol 1e-5
                        out0:1e-4 out1:1e-3. Note that the default tolerance
                        typically works well for FP32 but may be too strict
                        for lower precisions like FP16 or INT8.
  --atol ATOL [ATOL ...], --abs-tol ATOL [ATOL ...]
                        Absolute tolerance for output comparison. To specify
                        per-output tolerances, use the format: --atol
                        [<out_name>:]<atol>. If no output name is provided,
                        the tolerance is used for any outputs not explicitly
                        specified. For example: --atol 1e-5 out0:1e-4
                        out1:1e-3. Note that the default tolerance typically
                        works well for FP32 but may be too strict for lower
                        precisions like FP16 or INT8.
  --check-error-stat CHECK_ERROR_STAT [CHECK_ERROR_STAT ...]
                        The error statistic to check. For details on possible
                        values, see the documentation for
                        CompareFunc.simple(). To specify per-output values,
                        use the format: --check-error-stat
                        [<out_name>:]<stat>. If no output name is provided,
                        the value is used for any outputs not explicitly
                        specified. For example: --check-error-stat max
                        out0:mean out1:median
  --infinities-compare-equal
                        If set, then any matching +-inf values in outputs will
                        have an absdiff of 0. Otherwise, by default they will
                        have an absdiff of NaN.
  --save-heatmaps SAVE_HEATMAPS
                        [EXPERIMENTAL] Directory in which to save heatmaps of
                        the absolute and relative error.
  --show-heatmaps       [EXPERIMENTAL] Whether to display heatmaps of the
                        absolute and relative error. Defaults to False.
  --save-error-metrics-plot SAVE_ERROR_METRICS_PLOT
                        [EXPERIMENTAL] Path to directory to save error metrics
                        plot(s). If set, generates plot of absolute and
                        relative error against reference output magnitude.This
                        directory is created if it does not already exist.This
                        is useful for finding trends in errors, determining
                        whether accuracy failures are just outliers or deeper
                        problems.
  --show-error-metrics-plot
                        [EXPERIMENTAL] Whether to display the error metrics
                        plots. Defaults to False.
  --error-quantile ERROR_QUANTILE [ERROR_QUANTILE ...]
                        The error quantile to compare. Float, valid range [0,
                        1]To specify per-output values, use the format:
                        --quantile [<out_name>:]<stat>. If no output name is
                        provided, the value is used for any outputs not
                        explicitly specified. For example: --error-quantile
                        0.95 out0:0.8 out1:0.9

Comparison Function: `indices`:
  Options related to the `CompareFunc.indices` comparison function.

  --index-tolerance INDEX_TOLERANCE [INDEX_TOLERANCE ...]
                        Index tolerance for output comparison. For details on
                        what this means, see the API documentation for
                        `CompareFunc.indices()`. To specify per-output
                        tolerances, use the format: --index-tolerance
                        [<out_name>:]<index_tol>. If no output name is
                        provided, the tolerance is used for any outputs not
                        explicitly specified. For example: --index_tolerance 1
                        out0:0 out1:3.
