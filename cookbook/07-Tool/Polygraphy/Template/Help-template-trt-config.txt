usage: polygraphy template trt-config [-h] [-v] [-q]
                                      [--verbosity VERBOSITY [VERBOSITY ...]]
                                      [--silent]
                                      [--log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]]
                                      [--log-file LOG_FILE]
                                      [--model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}]
                                      [--input-shapes INPUT_SHAPES [INPUT_SHAPES ...]]
                                      [--seed SEED]
                                      [--val-range VAL_RANGE [VAL_RANGE ...]]
                                      [--int-min INT_MIN] [--int-max INT_MAX]
                                      [--float-min FLOAT_MIN]
                                      [--float-max FLOAT_MAX]
                                      [--iterations NUM]
                                      [--data-loader-backend-module {numpy,torch}]
                                      [--load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                                      | --data-loader-script
                                      DATA_LOADER_SCRIPT]
                                      [--data-loader-func-name DATA_LOADER_FUNC_NAME]
                                      [--trt-min-shapes TRT_MIN_SHAPES [TRT_MIN_SHAPES ...]]
                                      [--trt-opt-shapes TRT_OPT_SHAPES [TRT_OPT_SHAPES ...]]
                                      [--trt-max-shapes TRT_MAX_SHAPES [TRT_MAX_SHAPES ...]]
                                      [--tf32] [--fp16] [--bf16] [--fp8]
                                      [--int8]
                                      [--precision-constraints {prefer,obey,none}]
                                      [--sparse-weights]
                                      [--version-compatible]
                                      [--exclude-lean-runtime]
                                      [--calibration-cache CALIBRATION_CACHE]
                                      [--calib-base-cls CALIBRATION_BASE_CLASS]
                                      [--quantile QUANTILE]
                                      [--regression-cutoff REGRESSION_CUTOFF]
                                      [--load-timing-cache LOAD_TIMING_CACHE]
                                      [--error-on-timing-cache-miss]
                                      [--disable-compilation-cache]
                                      [--save-tactics SAVE_TACTICS | --load-tactics LOAD_TACTICS]
                                      [--tactic-sources [TACTIC_SOURCES ...]]
                                      [--trt-config-script TRT_CONFIG_SCRIPT]
                                      [--trt-config-func-name TRT_CONFIG_FUNC_NAME]
                                      [--trt-config-postprocess-script TRT_CONFIG_POSTPROCESS_SCRIPT]
                                      [--trt-safety-restricted] [--refittable]
                                      [--strip-plan] [--use-dla]
                                      [--allow-gpu-fallback]
                                      [--pool-limit MEMORY_POOL_LIMIT [MEMORY_POOL_LIMIT ...]]
                                      [--preview-features [PREVIEW_FEATURES ...]]
                                      [--builder-optimization-level BUILDER_OPTIMIZATION_LEVEL]
                                      [--hardware-compatibility-level HARDWARE_COMPATIBILITY_LEVEL]
                                      [--max-aux-streams MAX_AUX_STREAMS]
                                      [--quantization-flags [QUANTIZATION_FLAGS ...]]
                                      [--profiling-verbosity PROFILING_VERBOSITY]
                                      [--weight-streaming]
                                      [--runtime-platform RUNTIME_PLATFORM] -o
                                      OUTPUT
                                      [model_file]

Generate a template script to create a TensorRT builder configuration.

options:
  -h, --help            show this help message and exit
  -o OUTPUT, --output OUTPUT
                        Path to save the generated script template.

Logging:
  Options related to logging and debug output

  -v, --verbose         Increase logging verbosity. Specify multiple times for
                        higher verbosity
  -q, --quiet           Decrease logging verbosity. Specify multiple times for
                        lower verbosity
  --verbosity VERBOSITY [VERBOSITY ...]
                        The logging verbosity to use. Takes precedence over
                        the `-v` and `-q` options, and unlike them, allows you
                        to control per-path verbosity. Verbosity values should
                        come from Polygraphy's logging verbosities defined in
                        the `Logger` class and are case-insensitive. For
                        example: `--verbosity INFO` or `--verbosity verbose`.
                        To specify per-path verbosity, use the format:
                        `<path>:<verbosity>`. For example: `--verbosity
                        backend/trt:INFO backend/trt/loader.py:VERBOSE`. Paths
                        should be relative to the `polygraphy/` directory. For
                        example, `polygraphy/backend` should be specified with
                        just `backend`. The most closely matching path is used
                        to determine verbosity. For example, with:
                        `--verbosity warning backend:info
                        backend/trt:verbose`, a file under
                        `polygraphy/comparator` would use `WARNING` verbosity,
                        one under `backend/onnx` would use `INFO`, and one
                        under `backend/trt` would use `VERBOSE`.
  --silent              Disable all output
  --log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]
                        Format for log messages: {{'timestamp': Include
                        timestamp, 'line-info': Include file and line number,
                        'no-colors': Disable colors}}
  --log-file LOG_FILE   Path to a file where Polygraphy logging output should
                        be written. This may not include logging output from
                        dependencies, like TensorRT or ONNX-Runtime.

Model:
  Options related to the model

  model_file            Path to the model
  --model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}
                        The type of the input model: {{'frozen': TensorFlow
                        frozen graph; 'keras': Keras model; 'ckpt': TensorFlow
                        checkpoint directory; 'onnx': ONNX model; 'engine':
                        TensorRT engine; 'trt-network-script': A Python script
                        that defines a `load_network` function that takes no
                        arguments and returns a TensorRT Builder, Network, and
                        optionally Parser. If the function name is not
                        `load_network`, it can be specified after the model
                        file, separated by a colon. For example:
                        `my_custom_script.py:my_func`; 'uff': UFF file
                        [deprecated]; 'caffe': Caffe prototxt [deprecated]}}
  --input-shapes INPUT_SHAPES [INPUT_SHAPES ...], --inputs INPUT_SHAPES [INPUT_SHAPES ...]
                        Model input(s) and their shape(s). Used to determine
                        shapes to use while generating input data for
                        inference. Format: --input-shapes <name>:<shape>. For
                        example: --input-shapes image:[1,3,224,224]
                        other_input:[10]

Data Loader:
  Options related to loading or generating input data for inference.

  --seed SEED           Seed to use for random inputs
  --val-range VAL_RANGE [VAL_RANGE ...]
                        Range of values to generate in the data loader. To
                        specify per-input ranges, use the format: --val-range
                        <input_name>:[min,max]. If no input name is provided,
                        the range is used for any inputs not explicitly
                        specified. For example: --val-range [0,1] inp0:[2,50]
                        inp1:[3.0,4.6]
  --int-min INT_MIN     [DEPRECATED: Use --val-range] Minimum integer value
                        for random integer inputs
  --int-max INT_MAX     [DEPRECATED: Use --val-range] Maximum integer value
                        for random integer inputs
  --float-min FLOAT_MIN
                        [DEPRECATED: Use --val-range] Minimum float value for
                        random float inputs
  --float-max FLOAT_MAX
                        [DEPRECATED: Use --val-range] Maximum float value for
                        random float inputs
  --iterations NUM, --iters NUM
                        Number of inference iterations for which the default
                        data loader should supply data
  --data-loader-backend-module {numpy,torch}
                        The module to use for generating input arrays.
                        Currently supported options: numpy, torch
  --load-inputs LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...], --load-input-data LOAD_INPUTS_PATHS [LOAD_INPUTS_PATHS ...]
                        Path(s) to load inputs. The file(s) should be a JSON-
                        ified List[Dict[str, numpy.ndarray]], i.e. a list
                        where each element is the feed_dict for a single
                        iteration. When this option is used, all other data
                        loader arguments are ignored.
  --data-loader-script DATA_LOADER_SCRIPT
                        Path to a Python script that defines a function that
                        loads input data. The function should take no
                        arguments and return a generator or iterable that
                        yields input data (Dict[str, np.ndarray]). When this
                        option is used, all other data loader arguments are
                        ignored. By default, Polygraphy looks for a function
                        called `load_data`. You can specify a custom function
                        name by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --data-loader-func-name DATA_LOADER_FUNC_NAME
                        [DEPRECATED - function name can be specified with
                        --data-loader-script like so:
                        `my_custom_script.py:my_func`] When using a data-
                        loader-script, this specifies the name of the function
                        that loads data. Defaults to `load_data`.

TensorRT Builder Configuration:
  Options related to creating the TensorRT BuilderConfig.

  --trt-min-shapes TRT_MIN_SHAPES [TRT_MIN_SHAPES ...]
                        The minimum shapes the optimization profile(s) will
                        support. Specify this option once for each profile. If
                        not provided, inference-time input shapes are used.
                        Format: --trt-min-shapes <input0>:[D0,D1,..,DN] ..
                        <inputN>:[D0,D1,..,DN]
  --trt-opt-shapes TRT_OPT_SHAPES [TRT_OPT_SHAPES ...]
                        The shapes for which the optimization profile(s) will
                        be most performant. Specify this option once for each
                        profile. If not provided, inference-time input shapes
                        are used. Format: --trt-opt-shapes
                        <input0>:[D0,D1,..,DN] .. <inputN>:[D0,D1,..,DN]
  --trt-max-shapes TRT_MAX_SHAPES [TRT_MAX_SHAPES ...]
                        The maximum shapes the optimization profile(s) will
                        support. Specify this option once for each profile. If
                        not provided, inference-time input shapes are used.
                        Format: --trt-max-shapes <input0>:[D0,D1,..,DN] ..
                        <inputN>:[D0,D1,..,DN]
  --tf32                Enable tf32 precision in TensorRT
  --fp16                Enable fp16 precision in TensorRT
  --bf16                Enable bf16 precision in TensorRT
  --fp8                 Enable fp8 precision in TensorRT
  --int8                Enable int8 precision in TensorRT. If calibration is
                        required but no calibration cache is provided, this
                        option will cause TensorRT to run int8 calibration
                        using the Polygraphy data loader to provide
                        calibration data. If calibration is run and the model
                        has dynamic shapes, the last optimization profile will
                        be used as the calibration profile.
  --precision-constraints {prefer,obey,none}
                        If set to `prefer`, TensorRT will restrict available
                        tactics to layer precisions specified in the network
                        unless no implementation exists with the preferred
                        layer constraints, in which case it will issue a
                        warning and use the fastest available implementation.
                        If set to `obey`, TensorRT will instead fail to build
                        the network if no implementation exists with the
                        preferred layer constraints. Defaults to `none`
  --sparse-weights      Enable optimizations for sparse weights in TensorRT
  --version-compatible  Builds an engine designed to be forward TensorRT
                        version compatible.
  --exclude-lean-runtime
                        Exclude the lean runtime from the plan when version
                        compatibility is enabled.
  --calibration-cache CALIBRATION_CACHE
                        Path to load/save a calibration cache. Used to store
                        calibration scales to speed up the process of int8
                        calibration. If the provided path does not yet exist,
                        int8 calibration scales will be calculated and written
                        to it during engine building. If the provided path
                        does exist, it will be read and int8 calibration will
                        be skipped during engine building.
  --calib-base-cls CALIBRATION_BASE_CLASS, --calibration-base-class CALIBRATION_BASE_CLASS
                        The name of the calibration base class to use. For
                        example, 'IInt8MinMaxCalibrator'.
  --quantile QUANTILE   The quantile to use for IInt8LegacyCalibrator. Has no
                        effect for other calibrator types.
  --regression-cutoff REGRESSION_CUTOFF
                        The regression cutoff to use for
                        IInt8LegacyCalibrator. Has no effect for other
                        calibrator types.
  --load-timing-cache LOAD_TIMING_CACHE
                        Path to load tactic timing cache. Used to cache tactic
                        timing information to speed up the engine building
                        process. If the file specified by --load-timing-cache
                        does not exist, Polygraphy will emit a warning and
                        fall back to using an empty timing cache.
  --error-on-timing-cache-miss
                        Emit error when a tactic being timed is not present in
                        the timing cache.
  --disable-compilation-cache
                        Disable caching JIT-compiled code
  --save-tactics SAVE_TACTICS, --save-tactic-replay SAVE_TACTICS
                        Path to save a Polygraphy tactic replay file. Details
                        about tactics selected by TensorRT will be recorded
                        and stored at this location as a JSON file.
  --load-tactics LOAD_TACTICS, --load-tactic-replay LOAD_TACTICS
                        Path to load a Polygraphy tactic replay file, such as
                        one created by --save-tactics. The tactics specified
                        in the file will be used to override TensorRT's
                        default selections.
  --tactic-sources [TACTIC_SOURCES ...]
                        Tactic sources to enable. This controls which
                        libraries (e.g. cudnn, cublas, etc.) TensorRT is
                        allowed to load tactics from. Values come from the
                        names of the values in the trt.TacticSource enum and
                        are case-insensitive. If no arguments are provided,
                        e.g. '--tactic-sources', then all tactic sources are
                        disabled.Defaults to TensorRT's default tactic
                        sources.
  --trt-config-script TRT_CONFIG_SCRIPT
                        Path to a Python script that defines a function that
                        creates a TensorRT IBuilderConfig. The function should
                        take a builder and network as parameters and return a
                        TensorRT builder configuration. When this option is
                        specified, all other config arguments are ignored. By
                        default, Polygraphy looks for a function called
                        `load_config`. You can specify a custom function name
                        by separating it with a colon. For example:
                        `my_custom_script.py:my_func`
  --trt-config-func-name TRT_CONFIG_FUNC_NAME
                        [DEPRECATED - function name can be specified with
                        --trt-config-script like so:
                        `my_custom_script.py:my_func`]When using a trt-config-
                        script, this specifies the name of the function that
                        creates the config. Defaults to `load_config`.
  --trt-config-postprocess-script TRT_CONFIG_POSTPROCESS_SCRIPT, --trt-cpps TRT_CONFIG_POSTPROCESS_SCRIPT
                        [EXPERIMENTAL] Path to a Python script that defines a
                        function that modifies a TensorRT IBuilderConfig. This
                        function will be called after Polygraphy has finished
                        created the builder configuration and should take a
                        builder, network, and config as parameters and modify
                        the config in place. Unlike `--trt-config-script`, all
                        other config arguments will be reflected in the config
                        passed to the function.By default, Polygraphy looks
                        for a function called `postprocess_config`. You can
                        specify a custom function name by separating it with a
                        colon. For example: `my_custom_script.py:my_func`
  --trt-safety-restricted
                        Enable safety scope checking in TensorRT
  --refittable          Enable the engine to be refitted with new weights
                        after it is built.
  --strip-plan          Builds the engine with the refittable weights
                        stripped.
  --use-dla             [EXPERIMENTAL] Use DLA as the default device type
  --allow-gpu-fallback  [EXPERIMENTAL] Allow layers unsupported on the DLA to
                        fall back to GPU. Has no effect if --use-dla is not
                        set.
  --pool-limit MEMORY_POOL_LIMIT [MEMORY_POOL_LIMIT ...], --memory-pool-limit MEMORY_POOL_LIMIT [MEMORY_POOL_LIMIT ...]
                        Memory pool limits. Memory pool names come from the
                        names of values in the `trt.MemoryPoolType` enum and
                        are case-insensitiveFormat: `--pool-limit
                        <pool_name>:<pool_limit> ...`. For example, `--pool-
                        limit dla_local_dram:1e9 workspace:16777216`.
                        Optionally, use a `K`, `M`, or `G` suffix to indicate
                        KiB, MiB, or GiB respectively. For example, `--pool-
                        limit workspace:16M` is equivalent to `--pool-limit
                        workspace:16777216`.
  --preview-features [PREVIEW_FEATURES ...]
                        Preview features to enable. Values come from the names
                        of the values in the trt.PreviewFeature enum, and are
                        case-insensitive.If no arguments are provided, e.g. '
                        --preview-features', then all preview features are
                        disabled. Defaults to TensorRT's default preview
                        features.
  --builder-optimization-level BUILDER_OPTIMIZATION_LEVEL
                        The builder optimization level. Setting a higher
                        optimization level allows the optimizer to spend more
                        time searching for optimization opportunities. The
                        resulting engine may have better performance compared
                        to an engine built with a lower optimization level.
                        Refer to the TensorRT API documentation for details.
  --hardware-compatibility-level HARDWARE_COMPATIBILITY_LEVEL
                        The hardware compatibility level to use for the
                        engine. This allows engines built on one GPU
                        architecture to work on GPUs of other architectures.
                        Values come from the names of values in the
                        `trt.HardwareCompatibilityLevel` enum and are case-
                        insensitive. For example, `--hardware-compatibility-
                        level ampere_plus`
  --max-aux-streams MAX_AUX_STREAMS
                        The maximum number of auxiliary streams that TensorRT
                        is allowed to use. If the network contains operators
                        that can run in parallel, TRT can execute them using
                        auxiliary streams in addition to the one provided to
                        the IExecutionContext.execute_async_v3() call. The
                        default maximum number of auxiliary streams is
                        determined by the heuristics in TensorRT on whether
                        enabling multi-stream would improve the performance.
                        Refer to the TensorRT API documentation for details.
  --quantization-flags [QUANTIZATION_FLAGS ...]
                        Int8 quantization flags to enable. Values come from
                        the names of values in the trt.QuantizationFlag enum,
                        and are case-insensitive. If no arguments are
                        provided, e.g. '--quantization-flags', then all
                        quantization flags are disabled. Defaults to
                        TensorRT's default quantization flags.
  --profiling-verbosity PROFILING_VERBOSITY
                        The verbosity of NVTX annotations in the generated
                        engine.Values come from the names of values in the
                        `trt.ProfilingVerbosity` enum and are case-
                        insensitive. For example, `--profiling-verbosity
                        detailed`. Defaults to 'verbose'.
  --weight-streaming    Build a weight streamable engine. Must be set with
                        --strongly-typed. The weight streaming amount can be
                        set with --weight-streaming-budget.
  --runtime-platform RUNTIME_PLATFORM
                        The target runtime platform (operating system and CPU
                        architecture) for the execution of the TensorRT
                        engine. TensorRT provides support for cross-platform
                        engine compatibility when the target runtime platform
                        is different from the build platform. Values come from
                        the names of values in the `trt.RuntimePlatform` enum
                        and are case-insensitive. For example, `--runtime-
                        platform same_as_build`, `--runtime-platform
                        windows_amd64`
