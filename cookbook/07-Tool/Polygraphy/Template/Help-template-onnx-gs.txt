usage: polygraphy template onnx-gs [-h] [-v] [-q]
                                   [--verbosity VERBOSITY [VERBOSITY ...]]
                                   [--silent]
                                   [--log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]]
                                   [--log-file LOG_FILE]
                                   [--input-shapes INPUT_SHAPES [INPUT_SHAPES ...]]
                                   [--shape-inference]
                                   [--no-onnxruntime-shape-inference]
                                   [--external-data-dir EXTERNAL_DATA_DIR]
                                   [--ignore-external-data]
                                   [--onnx-outputs ONNX_OUTPUTS [ONNX_OUTPUTS ...]]
                                   [--onnx-exclude-outputs ONNX_EXCLUDE_OUTPUTS [ONNX_EXCLUDE_OUTPUTS ...]]
                                   [--fp-to-fp16] -o OUTPUT
                                   model_file

[EXPERIMENTAL] Generate a template script to process an ONNX model with ONNX-GraphSurgeon.

options:
  -h, --help            show this help message and exit
  -o OUTPUT, --output OUTPUT
                        Path to save the generated script template.

Logging:
  Options related to logging and debug output

  -v, --verbose         Increase logging verbosity. Specify multiple times for
                        higher verbosity
  -q, --quiet           Decrease logging verbosity. Specify multiple times for
                        lower verbosity
  --verbosity VERBOSITY [VERBOSITY ...]
                        The logging verbosity to use. Takes precedence over
                        the `-v` and `-q` options, and unlike them, allows you
                        to control per-path verbosity. Verbosity values should
                        come from Polygraphy's logging verbosities defined in
                        the `Logger` class and are case-insensitive. For
                        example: `--verbosity INFO` or `--verbosity verbose`.
                        To specify per-path verbosity, use the format:
                        `<path>:<verbosity>`. For example: `--verbosity
                        backend/trt:INFO backend/trt/loader.py:VERBOSE`. Paths
                        should be relative to the `polygraphy/` directory. For
                        example, `polygraphy/backend` should be specified with
                        just `backend`. The most closely matching path is used
                        to determine verbosity. For example, with:
                        `--verbosity warning backend:info
                        backend/trt:verbose`, a file under
                        `polygraphy/comparator` would use `WARNING` verbosity,
                        one under `backend/onnx` would use `INFO`, and one
                        under `backend/trt` would use `VERBOSE`.
  --silent              Disable all output
  --log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]
                        Format for log messages: {{'timestamp': Include
                        timestamp, 'line-info': Include file and line number,
                        'no-colors': Disable colors}}
  --log-file LOG_FILE   Path to a file where Polygraphy logging output should
                        be written. This may not include logging output from
                        dependencies, like TensorRT or ONNX-Runtime.

Model:
  Options related to the model

  model_file            Path to the model
  --input-shapes INPUT_SHAPES [INPUT_SHAPES ...], --inputs INPUT_SHAPES [INPUT_SHAPES ...]
                        Model input(s) and their shape(s). Used to determine
                        shapes to use while generating input data for
                        inference. Format: --input-shapes <name>:<shape>. For
                        example: --input-shapes image:[1,3,224,224]
                        other_input:[10]

ONNX Shape Inference:
  Options related to ONNX shape inference.

  --shape-inference, --do-shape-inference
                        Enable ONNX shape inference when loading the model
  --no-onnxruntime-shape-inference
                        Disable using ONNX-Runtime's shape inference
                        utilities. This will force Polygraphy to use
                        `onnx.shape_inference` instead. Note that ONNX-
                        Runtime's shape inference utilities may be more
                        performant and memory-efficient.

ONNX Model Loading:
  Options related to loading ONNX models.

  --external-data-dir EXTERNAL_DATA_DIR, --load-external-data EXTERNAL_DATA_DIR, --ext EXTERNAL_DATA_DIR
                        Path to a directory containing external data for the
                        model. Generally, this is only required if the
                        external data is not stored in the model directory.
  --ignore-external-data
                        Ignore external data and just load the model structure
                        without any weights. The model will be usable only for
                        purposes that don't require weights, such as
                        extracting subgraphs or inspecting model structure.
                        This can be useful in cases where external data is not
                        available.
  --onnx-outputs ONNX_OUTPUTS [ONNX_OUTPUTS ...]
                        Name(s) of ONNX tensor(s) to mark as output(s). Using
                        the special value 'mark all' indicates that all
                        tensors should be used as outputs
  --onnx-exclude-outputs ONNX_EXCLUDE_OUTPUTS [ONNX_EXCLUDE_OUTPUTS ...]
                        [EXPERIMENTAL] Name(s) of ONNX output(s) to unmark as
                        outputs.
  --fp-to-fp16          Convert all floating point tensors in an ONNX model to
                        16-bit precision. This is *not* needed in order to use
                        TensorRT's fp16 precision, but may be useful for other
                        backends. Requires onnxmltools.
