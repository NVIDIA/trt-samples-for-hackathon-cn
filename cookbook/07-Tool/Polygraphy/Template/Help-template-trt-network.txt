usage: polygraphy template trt-network [-h] [-v] [-q]
                                       [--verbosity VERBOSITY [VERBOSITY ...]]
                                       [--silent]
                                       [--log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]]
                                       [--log-file LOG_FILE]
                                       [--model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}]
                                       [--ckpt CKPT]
                                       [--tf-outputs TF_OUTPUTS [TF_OUTPUTS ...]]
                                       [--freeze-graph] [--opset OPSET]
                                       [--external-data-dir EXTERNAL_DATA_DIR]
                                       [--ignore-external-data]
                                       [--onnx-outputs ONNX_OUTPUTS [ONNX_OUTPUTS ...]]
                                       [--onnx-exclude-outputs ONNX_EXCLUDE_OUTPUTS [ONNX_EXCLUDE_OUTPUTS ...]]
                                       [--fp-to-fp16]
                                       [--plugins PLUGINS [PLUGINS ...]]
                                       [--trt-outputs TRT_OUTPUTS [TRT_OUTPUTS ...]]
                                       [--trt-exclude-outputs TRT_EXCLUDE_OUTPUTS [TRT_EXCLUDE_OUTPUTS ...]]
                                       [--layer-precisions LAYER_PRECISIONS [LAYER_PRECISIONS ...]]
                                       [--tensor-dtypes TENSOR_DTYPES [TENSOR_DTYPES ...]]
                                       [--trt-network-func-name TRT_NETWORK_FUNC_NAME]
                                       [--trt-network-postprocess-script TRT_NETWORK_POSTPROCESS_SCRIPT [TRT_NETWORK_POSTPROCESS_SCRIPT ...]]
                                       [--strongly-typed]
                                       [--mark-debug MARK_DEBUG [MARK_DEBUG ...]]
                                       [--onnx-flags ONNX_FLAGS [ONNX_FLAGS ...]]
                                       [--plugin-instancenorm] -o OUTPUT
                                       [model_file]

Generate a template script to create a TensorRT network using the TensorRT network API,
optionally starting from an existing model.

options:
  -h, --help            show this help message and exit
  -o OUTPUT, --output OUTPUT
                        Path to save the generated script template.

Logging:
  Options related to logging and debug output

  -v, --verbose         Increase logging verbosity. Specify multiple times for
                        higher verbosity
  -q, --quiet           Decrease logging verbosity. Specify multiple times for
                        lower verbosity
  --verbosity VERBOSITY [VERBOSITY ...]
                        The logging verbosity to use. Takes precedence over
                        the `-v` and `-q` options, and unlike them, allows you
                        to control per-path verbosity. Verbosity values should
                        come from Polygraphy's logging verbosities defined in
                        the `Logger` class and are case-insensitive. For
                        example: `--verbosity INFO` or `--verbosity verbose`.
                        To specify per-path verbosity, use the format:
                        `<path>:<verbosity>`. For example: `--verbosity
                        backend/trt:INFO backend/trt/loader.py:VERBOSE`. Paths
                        should be relative to the `polygraphy/` directory. For
                        example, `polygraphy/backend` should be specified with
                        just `backend`. The most closely matching path is used
                        to determine verbosity. For example, with:
                        `--verbosity warning backend:info
                        backend/trt:verbose`, a file under
                        `polygraphy/comparator` would use `WARNING` verbosity,
                        one under `backend/onnx` would use `INFO`, and one
                        under `backend/trt` would use `VERBOSE`.
  --silent              Disable all output
  --log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]
                        Format for log messages: {{'timestamp': Include
                        timestamp, 'line-info': Include file and line number,
                        'no-colors': Disable colors}}
  --log-file LOG_FILE   Path to a file where Polygraphy logging output should
                        be written. This may not include logging output from
                        dependencies, like TensorRT or ONNX-Runtime.

Model:
  Options related to the model

  model_file            Path to the model
  --model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}
                        The type of the input model: {{'frozen': TensorFlow
                        frozen graph; 'keras': Keras model; 'ckpt': TensorFlow
                        checkpoint directory; 'onnx': ONNX model; 'engine':
                        TensorRT engine; 'trt-network-script': A Python script
                        that defines a `load_network` function that takes no
                        arguments and returns a TensorRT Builder, Network, and
                        optionally Parser. If the function name is not
                        `load_network`, it can be specified after the model
                        file, separated by a colon. For example:
                        `my_custom_script.py:my_func`; 'uff': UFF file
                        [deprecated]; 'caffe': Caffe prototxt [deprecated]}}

TensorFlow Model Loading:
  Options related to loading TensorFlow models.

  --ckpt CKPT           [EXPERIMENTAL] Name of the checkpoint to load.
                        Required if the `checkpoint` file is missing. Should
                        not include file extension (e.g. to load `model.meta`
                        use `--ckpt=model`)
  --tf-outputs TF_OUTPUTS [TF_OUTPUTS ...]
                        Name(s) of TensorFlow output(s). Using '--tf-outputs
                        mark all' indicates that all tensors should be used as
                        outputs
  --freeze-graph        [EXPERIMENTAL] Attempt to freeze the graph

TensorFlow-ONNX Model Conversion:
  Options related to converting TensorFlow models to ONNX.

  --opset OPSET         Opset to use when converting to ONNX

ONNX Model Loading:
  Options related to loading ONNX models.

  --external-data-dir EXTERNAL_DATA_DIR, --load-external-data EXTERNAL_DATA_DIR, --ext EXTERNAL_DATA_DIR
                        Path to a directory containing external data for the
                        model. Generally, this is only required if the
                        external data is not stored in the model directory.
  --ignore-external-data
                        Ignore external data and just load the model structure
                        without any weights. The model will be usable only for
                        purposes that don't require weights, such as
                        extracting subgraphs or inspecting model structure.
                        This can be useful in cases where external data is not
                        available.
  --onnx-outputs ONNX_OUTPUTS [ONNX_OUTPUTS ...]
                        Name(s) of ONNX tensor(s) to mark as output(s). Using
                        the special value 'mark all' indicates that all
                        tensors should be used as outputs
  --onnx-exclude-outputs ONNX_EXCLUDE_OUTPUTS [ONNX_EXCLUDE_OUTPUTS ...]
                        [EXPERIMENTAL] Name(s) of ONNX output(s) to unmark as
                        outputs.
  --fp-to-fp16          Convert all floating point tensors in an ONNX model to
                        16-bit precision. This is *not* needed in order to use
                        TensorRT's fp16 precision, but may be useful for other
                        backends. Requires onnxmltools.

TensorRT Plugin Loading:
  Options related to loading TensorRT plugins.

  --plugins PLUGINS [PLUGINS ...]
                        Path(s) of plugin libraries to load

TensorRT Network Loading:
  Options related to loading TensorRT networks.

  --trt-outputs TRT_OUTPUTS [TRT_OUTPUTS ...]
                        Name(s) of TensorRT output(s). Using '--trt-outputs
                        mark all' indicates that all tensors should be used as
                        outputs
  --trt-exclude-outputs TRT_EXCLUDE_OUTPUTS [TRT_EXCLUDE_OUTPUTS ...]
                        [EXPERIMENTAL] Name(s) of TensorRT output(s) to unmark
                        as outputs.
  --layer-precisions LAYER_PRECISIONS [LAYER_PRECISIONS ...]
                        Compute precision to use for each layer. This should
                        be specified on a per-layer basis, using the format:
                        --layer-precisions <layer_name>:<layer_precision>.
                        Precision values come from the TensorRT data type
                        aliases, like float32, float16, int8, bool, etc. For
                        example: --layer-precisions example_layer:float16
                        other_layer:int8. When this option is provided, you
                        should also set --precision-constraints to either
                        'prefer' or 'obey'.
  --tensor-dtypes TENSOR_DTYPES [TENSOR_DTYPES ...], --tensor-datatypes TENSOR_DTYPES [TENSOR_DTYPES ...]
                        Data type to use for each network I/O tensor. This
                        should be specified on a per-tensor basis, using the
                        format: --tensor-datatypes
                        <tensor_name>:<tensor_datatype>. Data type values come
                        from the TensorRT data type aliases, like float32,
                        float16, int8, bool, etc. For example: --tensor-
                        datatypes example_tensor:float16 other_tensor:int8.
  --trt-network-func-name TRT_NETWORK_FUNC_NAME
                        [DEPRECATED - function name can be specified alongside
                        the script like so: `my_custom_script.py:my_func`]
                        When using a trt-network-script instead of other model
                        types, this specifies the name of the function that
                        loads the network. Defaults to `load_network`.
  --trt-network-postprocess-script TRT_NETWORK_POSTPROCESS_SCRIPT [TRT_NETWORK_POSTPROCESS_SCRIPT ...], --trt-npps TRT_NETWORK_POSTPROCESS_SCRIPT [TRT_NETWORK_POSTPROCESS_SCRIPT ...]
                        [EXPERIMENTAL] Specify a post-processing script to run
                        on the parsed TensorRT network. The script file may
                        optionally be suffixed with the name of the callable
                        to be invoked. For example: `--trt-npps
                        process.py:do_something`. If no callable is specified,
                        then by default Polygraphy uses the callable name
                        `postprocess`. The callable is expected to take a
                        named argument `network` of type
                        `trt.INetworkDefinition`. Multiple scripts may be
                        specified, in which case they are executed in the
                        order given.
  --strongly-typed      Mark the network as being strongly typed.
  --mark-debug MARK_DEBUG [MARK_DEBUG ...]
                        Specify list of names of tensors to be marked as debug
                        tensors.For example, `--mark-debug tensor1 tensor2
                        tensor3`.

ONNX-TRT Parser Flags:
  Options related to setting flags for TensorRT's ONNX parser

  --onnx-flags ONNX_FLAGS [ONNX_FLAGS ...]
                        Flag(s) for adjusting the default parsing behavior of
                        the ONNX parser.Flag values come from the
                        `trt.OnnxParserFlag` enum and are case-insensitve.For
                        example: --onnx-flags native_instancenorm
  --plugin-instancenorm
                        Switch to clear the
                        `trt.OnnxParserFlag.NATIVE_INSTANCENORM` flag andforce
                        the usage of the plugin implementation of ONNX
                        InstanceNorm.Note that
                        `trt.OnnxParserFlag.NATIVE_INSTANCENORM` is ON by
                        default since TensorRT 10.0.
