#pragma once

#include "utils.h"

template<typename T>
struct TypeConverter
{
    using Type = half2;
}; // keep for generality

template<>
struct TypeConverter<half2>
{
    using Type = half;
};

template<>
struct TypeConverter<half>
{
    using Type = half2;
};

template<typename T>
inline __device__ T hadd2(T a, T b)
{
    return __hadd2(a, b);
}

template<typename T>
inline __device__ T hmul2(T a, T b)
{
    return __hmul2(a, b);
}

template<typename T>
inline __device__ T hsub2(T a, T b)
{
    return __hsub2(a, b);
}

template<typename T>
inline __device__ T hexp2(T a)
{
    return h2exp(a);
}

template<typename T>
inline __device__ T ldg(const T *val)
{
    return __ldg(val);
}

template<typename T>
inline __device__ T float2type2(float a);

template<>
inline __device__ half2 float2type2(float a)
{
    return __float2half2_rn(a);
}

template<typename T_IN, typename T_OUT>
inline __device__ T_OUT type2type2(T_IN a);

template<>
inline __device__ half2 type2type2(half a)
{
    return __half2half2(a);
}

template<typename T, int NUM>
__inline__ __device__ T warpReduceMaxV2(T *val)
{
#pragma unroll
    for (int i = 0; i < NUM; i++)
    {
#pragma unroll
        for (int mask = 16; mask > 0; mask >>= 1)
            val[i] = max(val[i], __shfl_xor_sync(FINAL_MASK, val[i], mask, 32));
    }
    return (T)(0.0f);
}

template<typename T, int NUM>
__inline__ __device__ T blockReduceMaxV2(T *val)
{
    static __shared__ T shared[32][NUM];
    int                 lane = threadIdx.x & 0x1f; // in-warp idx
    int                 wid  = threadIdx.x >> 5;   // warp idx

    warpReduceMaxV2<T, NUM>(val);                  // get maxx in each warp

    if (lane == 0)                                 // record in-warp maxx by warp Idx
    {
#pragma unroll
        for (int i = 0; i < NUM; i++)
        {
            shared[wid][i] = val[i];
        }
    }

    __syncthreads();

    // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent
    // blockDim.x is not divided by 32
    bool is_mask = threadIdx.x < (blockDim.x / 32.f);
#pragma unroll
    for (int i = 0; i < NUM; i++)
    {
        val[i] = is_mask ? shared[lane][i] : (T)-1e20f;
    }
    warpReduceMaxV2<T, NUM>(val);

    return (T)0.0f;
}

template<typename T, int NUM>
__inline__ __device__ T warpReduceSumV2(T *val)
{
#pragma unroll
    for (int i = 0; i < NUM; i++)
    {
#pragma unroll
        for (int mask = 16; mask > 0; mask >>= 1)
            val[i] += __shfl_xor_sync(FINAL_MASK, val[i], mask, 32);
    }
    return (T)(0.0f);
}

template<typename T, int NUM>
__inline__ __device__ T blockReduceSumV2(T *val)
{
    static __shared__ T shared[NUM][33];
    int                 lane = threadIdx.x & 0x1f;
    int                 wid  = threadIdx.x >> 5;

    warpReduceSumV2<T, NUM>(val);

    if (lane == 0)
    {
#pragma unroll
        for (int i = 0; i < NUM; i++)
        {
            shared[i][wid] = val[i];
        }
    }

    __syncthreads();

    bool is_mask = threadIdx.x < (blockDim.x / 32.f);
#pragma unroll
    for (int i = 0; i < NUM; i++)
    {
        val[i] = is_mask ? shared[i][lane] : (T)(0.0f);
    }
    warpReduceSumV2<T, NUM>(val);
    return (T)0.0f;
}

template<typename T>
__inline__ __device__ T warpReduceSum(T val)
{
#pragma unroll
    for (int mask = 16; mask > 0; mask >>= 1)
        val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);
    return val;
}

/* Calculate the sum of all elements in a block */
template<typename T>
__inline__ __device__ T blockReduceSum(T val)
{
    static __shared__ T shared[32];
    int                 lane = threadIdx.x & 0x1f;
    int                 wid  = threadIdx.x >> 5;

    val = warpReduceSum<T>(val);

    if (lane == 0)
        shared[wid] = val;

    __syncthreads();

    // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent
    // blockDim.x is not divided by 32
    val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : (T)(0.0f);
    val = warpReduceSum<T>(val);

    return val;
}

template<typename T>
__inline__ __device__ T warpReduceMax(T val)
{
#pragma unroll
    for (int mask = 16; mask > 0; mask >>= 1)
        val = max(val, __shfl_xor_sync(FINAL_MASK, val, mask, 32));
    return val;
}

/* Calculate the maximum of all elements in a block */
template<typename T>
__inline__ __device__ T blockReduceMax(T val)
{
    static __shared__ T shared[32];
    int                 lane = threadIdx.x & 0x1f; // in-warp idx
    int                 wid  = threadIdx.x >> 5;   // warp idx

    val = warpReduceMax(val);                      // get maxx in each warp

    if (lane == 0)                                 // record in-warp maxx by warp Idx
        shared[wid] = val;

    __syncthreads();

    // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent
    // blockDim.x is not divided by 32
    val = (threadIdx.x < (blockDim.x / 32.f)) ? shared[lane] : -1e20f;
    val = warpReduceMax(val);

    return val;
}

// // Convert float to type (applied to half and bfloat16)
// template<typename T>
// inline __device__ T float2type(float a);

// template<>
// inline __device__ half float2type(float a) {
//     return __float2half_rn(a);
// }

template<typename T, typename T_IN>
void invokeAddMaskedSoftMax(T           *buffer,
                            const T_IN  *buffer_src,
                            const T_IN  *qp_buf,
                            const T     *attr_mask,
                            const int    batch_size,
                            const int    seq_len,
                            const int    head_num,
                            const T      scalar,
                            cudaStream_t stream);

template<typename T>
void invokeAddQKVPBiasTranspose(T           *q_buf,
                                T           *k_buf,
                                T           *v_buf,
                                T           *Q,
                                const T     *bias_Q,
                                T           *K,
                                const T     *bias_K,
                                T           *V,
                                const T     *bias_V,
                                T           *p_buf,
                                T           *P,
                                T           *q_buf_bias_v,
                                const T     *pos_bias_u,
                                const T     *pos_bias_v,
                                const int    batch_size,
                                const int    seq_len,
                                const int    head_num,
                                const int    size_per_head,
                                cudaStream_t stream);
